mc.LOAD_PRETRAINED_MODEL is True
ncluster/block is 8
nhWeight/block is 13
!!!!!!!!!!!nBlock!!!!! see this !!!61
!!!!!!!!sparse ratio!!!!!!!! see this !!!7.2
blocked!!!
!!!!!!!wantto!!!!!!!!!! see this !!!0.0
!!!!!!!!!but!!!!!! see this !!!0.0
1.TKTKTKTK::::::: [[-4.2524611e-04 -7.8560552e-05  1.0557692e-04 ... -7.8560552e-05
   1.0557692e-04 -4.2524611e-04]
 [ 2.8242654e-04 -7.8560552e-05 -2.3917720e-04 ...  2.8242654e-04
   1.0557692e-04  1.0557692e-04]
 [-7.8560552e-05  2.8242654e-04 -2.3917720e-04 ...  5.1783299e-04
  -2.3917720e-04 -2.3917720e-04]
 ...
 [-2.0617759e-04  2.0118323e-04  2.0118323e-04 ...  1.2099807e-07
   1.2099807e-07  1.2099807e-07]
 [ 1.2099807e-07  2.0118323e-04  2.0118323e-04 ...  1.2099807e-07
  -4.8814609e-04  1.2099807e-07]
 [ 2.0118323e-04  4.7122134e-04  1.2099807e-07 ...  1.2099807e-07
   9.0683199e-04  2.0118323e-04]]
2.TKTKTKTK::::::: [[-4.25246108e-04 -7.85605516e-05  1.05576917e-04 ... -7.85605516e-05
   1.05576917e-04 -4.25246108e-04]
 [ 2.82426539e-04 -7.85605516e-05 -2.39177200e-04 ...  2.82426539e-04
   1.05576917e-04  1.05576917e-04]
 [-7.85605516e-05  2.82426539e-04 -2.39177200e-04 ...  5.17832988e-04
  -2.39177200e-04 -2.39177200e-04]
 ...
 [-2.06177589e-04  2.01183226e-04  2.01183226e-04 ...  1.20998067e-07
   1.20998067e-07  1.20998067e-07]
 [ 1.20998067e-07  2.01183226e-04  2.01183226e-04 ...  1.20998067e-07
  -4.88146092e-04  1.20998067e-07]
 [ 2.01183226e-04  4.71221341e-04  1.20998067e-07 ...  1.20998067e-07
   9.06831992e-04  2.01183226e-04]]
3.TKTKTKTK::::::: [[  1   3   4 ...   3   4   1]
 [  5   3   2 ...   5   4   4]
 [  3   5   2 ...   6   2   2]
 ...
 [484 485 485 ... 486 486 486]
 [486 485 485 ... 486 482 486]
 [485 483 486 ... 486 481 485]]
ncluster/block is 8
nhWeight/block is 200
!!!!!!!!!!!nBlock!!!!! see this !!!5
!!!!!!!!sparse ratio!!!!!!!! see this !!!7.2
blocked!!!
!!!!!!!wantto!!!!!!!!!! see this !!!0.0
!!!!!!!!!but!!!!!! see this !!!0.0
1.TKTKTKTK::::::: [[-0.01030892 -0.01030892  0.08761837 ... -0.01030892 -0.01030892
   0.01438764]
 [ 0.08761837 -0.04682746  0.01438764 ... -0.04682746 -0.01030892
  -0.01030892]
 [ 0.01438764 -0.01030892 -0.01030892 ... -0.01030892 -0.01030892
  -0.01030892]
 ...
 [-0.06457908 -0.06457908 -0.06457908 ... -0.06457908 -0.06457908
   0.03455478]
 [-0.00771583 -0.00771583 -0.06457908 ... -0.00771583 -0.00771583
  -0.00771583]
 [-0.00771583 -0.00771583 -0.06457908 ... -0.00771583 -0.00771583
  -0.06457908]]
2.TKTKTKTK::::::: [[-0.01030892 -0.01030892  0.08761837 ... -0.01030892 -0.01030892
   0.01438764]
 [ 0.08761837 -0.04682746  0.01438764 ... -0.04682746 -0.01030892
  -0.01030892]
 [ 0.01438764 -0.01030892 -0.01030892 ... -0.01030892 -0.01030892
  -0.01030892]
 ...
 [-0.06457908 -0.06457908 -0.06457908 ... -0.06457908 -0.06457908
   0.03455478]
 [-0.00771583 -0.00771583 -0.06457908 ... -0.00771583 -0.00771583
  -0.00771583]
 [-0.00771583 -0.00771583 -0.06457908 ... -0.00771583 -0.00771583
  -0.06457908]]
3.TKTKTKTK::::::: [[ 4  4  5 ...  4  4  1]
 [ 5  3  1 ...  3  4  4]
 [ 1  4  4 ...  4  4  4]
 ...
 [34 34 34 ... 34 34 37]
 [38 38 34 ... 38 38 38]
 [38 38 34 ... 38 38 34]]
debug1.................................................add_forward__graph : end
debug2.................................................add_loss_graph : end
tk:grads_vars is  [(<tf.Tensor 'gradients/AddN_1:0' shape=(784, 1000) dtype=float32>, <tf.Variable 'dense1/weights:0' shape=(784, 1000) dtype=float32_ref>), (<tf.Tensor 'gradients/dense1/BiasAdd_grad/tuple/control_dependency_1:0' shape=(1000,) dtype=float32>, <tf.Variable 'dense1/biases:0' shape=(1000,) dtype=float32_ref>), (<tf.Tensor 'gradients/AddN:0' shape=(1000, 10) dtype=float32>, <tf.Variable 'dense2/weights:0' shape=(1000, 10) dtype=float32_ref>), (<tf.Tensor 'gradients/dense2/BiasAdd_grad/tuple/control_dependency_1:0' shape=(10,) dtype=float32>, <tf.Variable 'dense2/biases:0' shape=(10,) dtype=float32_ref>)]
0 trainable variable is !!!grad: Tensor("gradients/AddN_1:0", shape=(784, 1000), dtype=float32, device=/device:GPU:0)
0 trainable variable is !!!var: <tf.Variable 'dense1/weights:0' shape=(784, 1000) dtype=float32_ref>
1 trainable variable is !!!grad: Tensor("gradients/dense1/BiasAdd_grad/tuple/control_dependency_1:0", shape=(1000,), dtype=float32, device=/device:GPU:0)
1 trainable variable is !!!var: <tf.Variable 'dense1/biases:0' shape=(1000,) dtype=float32_ref>
2 trainable variable is !!!grad: Tensor("gradients/AddN:0", shape=(1000, 10), dtype=float32, device=/device:GPU:0)
2 trainable variable is !!!var: <tf.Variable 'dense2/weights:0' shape=(1000, 10) dtype=float32_ref>
3 trainable variable is !!!grad: Tensor("gradients/dense2/BiasAdd_grad/tuple/control_dependency_1:0", shape=(10,), dtype=float32, device=/device:GPU:0)
3 trainable variable is !!!var: <tf.Variable 'dense2/biases:0' shape=(10,) dtype=float32_ref>
debug3...............................................add_train_graph : end
Successfully Imported ../data/MNIST/train-images.gz
Extracting ../data/MNIST/train-images.gz
Successfully Imported ../data/MNIST/train-labels.gz
Extracting ../data/MNIST/train-labels.gz
Successfully Imported ../data/MNIST/test-images.gz
Extracting ../data/MNIST/test-images.gz
Successfully Imported ../data/MNIST/test-labels.gz
Extracting ../data/MNIST/test-labels.gz
Accuracy :  0.9182000058889389
-----------what is loss : 0.4666944146156311
2019-11-12 15:05:55.736383: step 0, loss = 0.47 (952.1 images/sec; 0.105 sec/batch)
-----------what is loss : 0.5107666254043579
2019-11-12 15:05:55.883703: step 10, loss = 0.51 (3466.1 images/sec; 0.029 sec/batch)
-----------what is loss : 0.5611176490783691
2019-11-12 15:05:56.004400: step 20, loss = 0.56 (3403.8 images/sec; 0.029 sec/batch)
-----------what is loss : 0.4367562532424927
2019-11-12 15:05:56.127377: step 30, loss = 0.44 (3258.6 images/sec; 0.031 sec/batch)
-----------what is loss : 0.4507865309715271
2019-11-12 15:05:56.248119: step 40, loss = 0.45 (3438.3 images/sec; 0.029 sec/batch)
-----------what is loss : 0.4902198314666748
2019-11-12 15:05:56.368372: step 50, loss = 0.49 (3432.0 images/sec; 0.029 sec/batch)
-----------what is loss : 0.5888897180557251
2019-11-12 15:05:56.488372: step 60, loss = 0.59 (3450.6 images/sec; 0.029 sec/batch)
-----------what is loss : 0.38971084356307983
2019-11-12 15:05:56.608026: step 70, loss = 0.39 (3399.2 images/sec; 0.029 sec/batch)
-----------what is loss : 0.38429927825927734
2019-11-12 15:05:56.727438: step 80, loss = 0.38 (3426.7 images/sec; 0.029 sec/batch)
-----------what is loss : 0.5808641910552979
2019-11-12 15:05:56.846591: step 90, loss = 0.58 (3466.9 images/sec; 0.029 sec/batch)
-----------what is loss : 0.43798384070396423
2019-11-12 15:05:56.965457: step 100, loss = 0.44 (3418.1 images/sec; 0.029 sec/batch)
-----------what is loss : 0.5753718614578247
2019-11-12 15:05:57.084169: step 110, loss = 0.58 (3507.8 images/sec; 0.029 sec/batch)
-----------what is loss : 0.4722837209701538
2019-11-12 15:05:57.204935: step 120, loss = 0.47 (3398.2 images/sec; 0.029 sec/batch)
-----------what is loss : 0.638703465461731
2019-11-12 15:05:57.323586: step 130, loss = 0.64 (3462.1 images/sec; 0.029 sec/batch)
-----------what is loss : 0.651745080947876
2019-11-12 15:05:57.443830: step 140, loss = 0.65 (3438.4 images/sec; 0.029 sec/batch)
-----------what is loss : 0.45768243074417114
2019-11-12 15:05:57.564356: step 150, loss = 0.46 (3326.7 images/sec; 0.030 sec/batch)
-----------what is loss : 0.5753786563873291
2019-11-12 15:05:57.684816: step 160, loss = 0.58 (3389.5 images/sec; 0.030 sec/batch)
-----------what is loss : 0.4858734607696533
2019-11-12 15:05:57.806760: step 170, loss = 0.49 (3424.6 images/sec; 0.029 sec/batch)
-----------what is loss : 0.5847828388214111
2019-11-12 15:05:57.925883: step 180, loss = 0.58 (3465.2 images/sec; 0.029 sec/batch)
-----------what is loss : 0.5477762818336487
2019-11-12 15:05:58.045573: step 190, loss = 0.55 (3419.2 images/sec; 0.029 sec/batch)
-----------what is loss : 0.4697827994823456
2019-11-12 15:05:58.165260: step 200, loss = 0.47 (3426.0 images/sec; 0.029 sec/batch)
-----------what is loss : 0.5236309766769409
2019-11-12 15:05:58.285600: step 210, loss = 0.52 (3387.3 images/sec; 0.030 sec/batch)
-----------what is loss : 0.5701318979263306
2019-11-12 15:05:58.412230: step 220, loss = 0.57 (2884.7 images/sec; 0.035 sec/batch)
-----------what is loss : 0.42450523376464844
2019-11-12 15:05:58.532385: step 230, loss = 0.42 (3387.0 images/sec; 0.030 sec/batch)
-----------what is loss : 0.4830242991447449
2019-11-12 15:05:58.651240: step 240, loss = 0.48 (3462.7 images/sec; 0.029 sec/batch)
-----------what is loss : 0.5522849559783936
2019-11-12 15:05:58.772535: step 250, loss = 0.55 (3292.7 images/sec; 0.030 sec/batch)
-----------what is loss : 0.41740697622299194
2019-11-12 15:05:58.957384: step 260, loss = 0.42 (1062.4 images/sec; 0.094 sec/batch)
-----------what is loss : 0.3801729083061218
2019-11-12 15:05:59.079046: step 270, loss = 0.38 (3422.1 images/sec; 0.029 sec/batch)
-----------what is loss : 0.4928814172744751
2019-11-12 15:05:59.199724: step 280, loss = 0.49 (3340.3 images/sec; 0.030 sec/batch)
-----------what is loss : 0.4864340126514435
2019-11-12 15:05:59.321074: step 290, loss = 0.49 (3376.4 images/sec; 0.030 sec/batch)
-----------what is loss : 0.45273342728614807
2019-11-12 15:05:59.441548: step 300, loss = 0.45 (3456.4 images/sec; 0.029 sec/batch)
-----------what is loss : 0.5320347547531128
2019-11-12 15:05:59.562219: step 310, loss = 0.53 (3331.9 images/sec; 0.030 sec/batch)
-----------what is loss : 0.49947255849838257
2019-11-12 15:05:59.682357: step 320, loss = 0.50 (3424.2 images/sec; 0.029 sec/batch)
-----------what is loss : 0.4505581855773926
2019-11-12 15:05:59.801288: step 330, loss = 0.45 (3440.2 images/sec; 0.029 sec/batch)
-----------what is loss : 0.6138858199119568
2019-11-12 15:05:59.921109: step 340, loss = 0.61 (3425.5 images/sec; 0.029 sec/batch)
-----------what is loss : 0.5491861701011658
2019-11-12 15:06:00.042933: step 350, loss = 0.55 (3406.0 images/sec; 0.029 sec/batch)
-----------what is loss : 0.5592473745346069
2019-11-12 15:06:00.163612: step 360, loss = 0.56 (3387.2 images/sec; 0.030 sec/batch)
-----------what is loss : 0.5590688586235046
2019-11-12 15:06:00.284680: step 370, loss = 0.56 (3440.8 images/sec; 0.029 sec/batch)
-----------what is loss : 0.4734729528427124
2019-11-12 15:06:00.405920: step 380, loss = 0.47 (3452.0 images/sec; 0.029 sec/batch)
-----------what is loss : 0.47759032249450684
2019-11-12 15:06:00.525731: step 390, loss = 0.48 (3394.1 images/sec; 0.029 sec/batch)
-----------what is loss : 0.492378294467926
2019-11-12 15:06:00.645207: step 400, loss = 0.49 (3381.1 images/sec; 0.030 sec/batch)
-----------what is loss : 0.54566490650177
2019-11-12 15:06:00.765901: step 410, loss = 0.55 (3420.0 images/sec; 0.029 sec/batch)
-----------what is loss : 0.517989456653595
2019-11-12 15:06:00.885344: step 420, loss = 0.52 (3477.4 images/sec; 0.029 sec/batch)
-----------what is loss : 0.6577873229980469
2019-11-12 15:06:01.006543: step 430, loss = 0.66 (3433.8 images/sec; 0.029 sec/batch)
-----------what is loss : 0.5752437710762024
2019-11-12 15:06:01.131943: step 440, loss = 0.58 (2909.4 images/sec; 0.034 sec/batch)
-----------what is loss : 0.47706174850463867
2019-11-12 15:06:01.252881: step 450, loss = 0.48 (3386.0 images/sec; 0.030 sec/batch)
-----------what is loss : 0.5163086652755737
2019-11-12 15:06:01.372295: step 460, loss = 0.52 (3444.2 images/sec; 0.029 sec/batch)
-----------what is loss : 0.5997865200042725
2019-11-12 15:06:01.493118: step 470, loss = 0.60 (3391.7 images/sec; 0.029 sec/batch)
-----------what is loss : 0.4878487288951874
2019-11-12 15:06:01.620603: step 480, loss = 0.49 (2871.7 images/sec; 0.035 sec/batch)
-----------what is loss : 0.5175390243530273
2019-11-12 15:06:01.741184: step 490, loss = 0.52 (3421.3 images/sec; 0.029 sec/batch)
-----------what is loss : 0.5798441171646118
2019-11-12 15:06:01.861797: step 500, loss = 0.58 (3371.8 images/sec; 0.030 sec/batch)
-----------what is loss : 0.4351362884044647
2019-11-12 15:06:01.980899: step 510, loss = 0.44 (3381.7 images/sec; 0.030 sec/batch)
-----------what is loss : 0.6036489009857178
2019-11-12 15:06:02.101505: step 520, loss = 0.60 (3377.4 images/sec; 0.030 sec/batch)
-----------what is loss : 0.5014133453369141
2019-11-12 15:06:02.277947: step 530, loss = 0.50 (3413.6 images/sec; 0.029 sec/batch)
-----------what is loss : 0.5070959329605103
2019-11-12 15:06:02.397694: step 540, loss = 0.51 (3430.5 images/sec; 0.029 sec/batch)
-----------what is loss : 0.5249565839767456
2019-11-12 15:06:02.518724: step 550, loss = 0.52 (3358.3 images/sec; 0.030 sec/batch)
-----------what is loss : 0.5173511505126953
2019-11-12 15:06:02.637361: step 560, loss = 0.52 (3441.5 images/sec; 0.029 sec/batch)
-----------what is loss : 0.4916574954986572
2019-11-12 15:06:02.757651: step 570, loss = 0.49 (3383.9 images/sec; 0.030 sec/batch)
-----------what is loss : 0.5446677207946777
2019-11-12 15:06:02.877770: step 580, loss = 0.54 (3451.6 images/sec; 0.029 sec/batch)
-----------what is loss : 0.4788386821746826
2019-11-12 15:06:02.998085: step 590, loss = 0.48 (3376.1 images/sec; 0.030 sec/batch)
-----------what is loss : 0.5588560700416565
2019-11-12 15:06:03.118813: step 600, loss = 0.56 (3361.2 images/sec; 0.030 sec/batch)
-----------what is loss : 0.4992142617702484
2019-11-12 15:06:03.238489: step 610, loss = 0.50 (3423.8 images/sec; 0.029 sec/batch)
-----------what is loss : 0.6019872426986694
2019-11-12 15:06:03.359021: step 620, loss = 0.60 (3395.0 images/sec; 0.029 sec/batch)
-----------what is loss : 0.5910468101501465
2019-11-12 15:06:03.479777: step 630, loss = 0.59 (3411.0 images/sec; 0.029 sec/batch)
-----------what is loss : 0.5669714212417603
2019-11-12 15:06:03.599984: step 640, loss = 0.57 (3426.2 images/sec; 0.029 sec/batch)
-----------what is loss : 0.5535440444946289
2019-11-12 15:06:03.720667: step 650, loss = 0.55 (3412.7 images/sec; 0.029 sec/batch)
-----------what is loss : 0.6045588254928589
2019-11-12 15:06:03.841328: step 660, loss = 0.60 (3384.7 images/sec; 0.030 sec/batch)
-----------what is loss : 0.5833344459533691
2019-11-12 15:06:03.960380: step 670, loss = 0.58 (3416.7 images/sec; 0.029 sec/batch)
-----------what is loss : 0.4002155065536499
2019-11-12 15:06:04.080555: step 680, loss = 0.40 (3434.1 images/sec; 0.029 sec/batch)
-----------what is loss : 0.6398022174835205
2019-11-12 15:06:04.201636: step 690, loss = 0.64 (3372.6 images/sec; 0.030 sec/batch)
-----------what is loss : 0.5457333326339722
2019-11-12 15:06:04.327490: step 700, loss = 0.55 (2865.5 images/sec; 0.035 sec/batch)
-----------what is loss : 0.5235291719436646
2019-11-12 15:06:04.447160: step 710, loss = 0.52 (3430.3 images/sec; 0.029 sec/batch)
-----------what is loss : 0.6008374691009521
2019-11-12 15:06:04.568146: step 720, loss = 0.60 (3359.6 images/sec; 0.030 sec/batch)
-----------what is loss : 0.544079065322876
2019-11-12 15:06:04.688381: step 730, loss = 0.54 (3457.7 images/sec; 0.029 sec/batch)
-----------what is loss : 0.5382823348045349
2019-11-12 15:06:04.806844: step 740, loss = 0.54 (3451.6 images/sec; 0.029 sec/batch)
-----------what is loss : 0.5265429019927979
2019-11-12 15:06:04.933320: step 750, loss = 0.53 (2808.1 images/sec; 0.036 sec/batch)
-----------what is loss : 0.5548129081726074
2019-11-12 15:06:05.053629: step 760, loss = 0.55 (3405.5 images/sec; 0.029 sec/batch)
-----------what is loss : 0.5587058067321777
2019-11-12 15:06:05.174482: step 770, loss = 0.56 (3411.4 images/sec; 0.029 sec/batch)
-----------what is loss : 0.4087212383747101
2019-11-12 15:06:05.295313: step 780, loss = 0.41 (3448.3 images/sec; 0.029 sec/batch)
-----------what is loss : 0.5426890254020691
2019-11-12 15:06:05.470404: step 790, loss = 0.54 (3456.9 images/sec; 0.029 sec/batch)
-----------what is loss : 0.6768719553947449
2019-11-12 15:06:05.591239: step 800, loss = 0.68 (3375.4 images/sec; 0.030 sec/batch)
-----------what is loss : 0.4001612663269043
2019-11-12 15:06:05.711036: step 810, loss = 0.40 (3428.9 images/sec; 0.029 sec/batch)
-----------what is loss : 0.5555031895637512
2019-11-12 15:06:05.830477: step 820, loss = 0.56 (3417.2 images/sec; 0.029 sec/batch)
-----------what is loss : 0.5572681427001953
2019-11-12 15:06:05.957134: step 830, loss = 0.56 (2839.5 images/sec; 0.035 sec/batch)
-----------what is loss : 0.44422638416290283
2019-11-12 15:06:06.077321: step 840, loss = 0.44 (3427.5 images/sec; 0.029 sec/batch)
-----------what is loss : 0.5254528522491455
2019-11-12 15:06:06.197267: step 850, loss = 0.53 (3431.1 images/sec; 0.029 sec/batch)
-----------what is loss : 0.5326994061470032
2019-11-12 15:06:06.317612: step 860, loss = 0.53 (3405.6 images/sec; 0.029 sec/batch)
-----------what is loss : 0.4941627085208893
2019-11-12 15:06:06.438761: step 870, loss = 0.49 (3283.5 images/sec; 0.030 sec/batch)
-----------what is loss : 0.637347936630249
2019-11-12 15:06:06.558417: step 880, loss = 0.64 (3395.0 images/sec; 0.029 sec/batch)
-----------what is loss : 0.4958013892173767
2019-11-12 15:06:06.678711: step 890, loss = 0.50 (3396.7 images/sec; 0.029 sec/batch)
-----------what is loss : 0.517813503742218
2019-11-12 15:06:06.798974: step 900, loss = 0.52 (3430.1 images/sec; 0.029 sec/batch)
-----------what is loss : 0.5791586637496948
2019-11-12 15:06:06.919768: step 910, loss = 0.58 (3430.6 images/sec; 0.029 sec/batch)
-----------what is loss : 0.4765914976596832
2019-11-12 15:06:07.039883: step 920, loss = 0.48 (3406.7 images/sec; 0.029 sec/batch)
-----------what is loss : 0.5188283920288086
2019-11-12 15:06:07.159085: step 930, loss = 0.52 (3449.1 images/sec; 0.029 sec/batch)
-----------what is loss : 0.45475858449935913
2019-11-12 15:06:07.278820: step 940, loss = 0.45 (3391.2 images/sec; 0.029 sec/batch)
-----------what is loss : 0.5281838178634644
2019-11-12 15:06:07.400037: step 950, loss = 0.53 (3437.4 images/sec; 0.029 sec/batch)
-----------what is loss : 0.5989454984664917
2019-11-12 15:06:07.520560: step 960, loss = 0.60 (3440.7 images/sec; 0.029 sec/batch)
-----------what is loss : 0.5036139488220215
2019-11-12 15:06:07.639635: step 970, loss = 0.50 (3462.3 images/sec; 0.029 sec/batch)
-----------what is loss : 0.6299389004707336
2019-11-12 15:06:07.759751: step 980, loss = 0.63 (3398.0 images/sec; 0.029 sec/batch)
-----------what is loss : 0.5211986303329468
2019-11-12 15:06:07.881299: step 990, loss = 0.52 (3364.2 images/sec; 0.030 sec/batch)
optimization Finished
Accuracy :  0.9184000045061111
tuple_tem key is dense2
tuple_tem key is dense1
tuple_tem value is (1000, 10)
tuple_tem value is (10,)
tuple_tem value is (784, 1000)
tuple_tem value is (1000,)
tk is  [[-2.33525861e-04 -4.40197255e-05  5.87345203e-05 ... -4.40197255e-05
   5.87345203e-05 -2.33525861e-04]
 [ 1.54350419e-04 -4.40197255e-05 -1.30853732e-04 ...  1.54350419e-04
   5.87345203e-05  5.87345203e-05]
 [-4.40197255e-05  1.54350419e-04 -1.30853732e-04 ...  2.89443793e-04
  -1.30853732e-04 -1.30853732e-04]
 ...
 [-1.13381655e-04  1.10635083e-04  1.10635083e-04 ...  6.65395063e-08
   6.65395063e-08  6.65395063e-08]
 [ 6.65395063e-08  1.10635083e-04  1.10635083e-04 ...  6.65395063e-08
  -2.68441974e-04  6.65395063e-08]
 [ 1.10635083e-04  2.59134889e-04  6.65395063e-08 ...  6.65395063e-08
   4.98686743e-04  1.10635083e-04]]
tk is  [[  1   3   4 ...   3   4   1]
 [  5   3   2 ...   5   4   4]
 [  3   5   2 ...   6   2   2]
 ...
 [484 485 485 ... 486 486 486]
 [486 485 485 ... 486 482 486]
 [485 483 486 ... 486 481 485]]
tk is  [[-0.00805988 -0.00805988  0.09721942 ... -0.00805988 -0.00805988
   0.01591353]
 [ 0.09721942 -0.04617394  0.01591353 ... -0.04617394 -0.00805988
  -0.00805988]
 [ 0.01591353 -0.00805988 -0.00805988 ... -0.00805988 -0.00805988
  -0.00805988]
 ...
 [-0.06506478 -0.06506478 -0.06506478 ... -0.06506478 -0.06506478
   0.0387761 ]
 [-0.00596215 -0.00596215 -0.06506478 ... -0.00596215 -0.00596215
  -0.00596215]
 [-0.00596215 -0.00596215 -0.06506478 ... -0.00596215 -0.00596215
  -0.06506478]]
tk is  [[ 4  4  5 ...  4  4  1]
 [ 5  3  1 ...  3  4  4]
 [ 1  4  4 ...  4  4  4]
 ...
 [34 34 34 ... 34 34 37]
 [38 38 34 ... 38 38 38]
 [38 38 34 ... 38 38 34]]
tk2 is  True
tk2 is  5
=============================================0.75===========================================
mc.LOAD_PRETRAINED_MODEL is True
OK to save ./MNIST/train/MNIST_PRUNING_INFO.pkl
ncluster/block is 8
nhWeight/block is 13
!!!!!!!!!!!nBlock!!!!! see this !!!61
!!!!!!!!sparse ratio!!!!!!!! see this !!!6.0
blocked!!!
!!!!!!!wantto!!!!!!!!!! see this !!!0.25
!!!!!!!!!but!!!!!! see this !!!0.010200255102040817
1.TKTKTKTK::::::: [[-2.33525861e-04 -4.40197255e-05  5.87345203e-05 ... -4.40197255e-05
   5.87345203e-05 -2.33525861e-04]
 [ 1.54350419e-04 -4.40197255e-05 -1.30853732e-04 ...  1.54350419e-04
   5.87345203e-05  5.87345203e-05]
 [-4.40197255e-05  1.54350419e-04 -1.30853732e-04 ...  2.89443793e-04
  -1.30853732e-04 -1.30853732e-04]
 ...
 [-1.13381655e-04  1.10635083e-04  1.10635083e-04 ...  6.65395063e-08
   6.65395063e-08  6.65395063e-08]
 [ 6.65395063e-08  1.10635083e-04  1.10635083e-04 ...  6.65395063e-08
  -2.68441974e-04  6.65395063e-08]
 [ 1.10635083e-04  2.59134889e-04  6.65395063e-08 ...  6.65395063e-08
   4.98686743e-04  1.10635083e-04]]
2.TKTKTKTK::::::: [[-2.33525861e-04 -4.40197255e-05  5.87345203e-05 ... -4.40197255e-05
   5.87345203e-05 -2.33525861e-04]
 [ 1.54350419e-04 -4.40197255e-05 -1.30853732e-04 ...  1.54350419e-04
   5.87345203e-05  5.87345203e-05]
 [-4.40197255e-05  1.54350419e-04 -1.30853732e-04 ...  2.89443793e-04
  -1.30853732e-04 -1.30853732e-04]
 ...
 [-1.13381655e-04  1.10635083e-04  1.10635083e-04 ...  6.65395063e-08
   6.65395063e-08  6.65395063e-08]
 [ 6.65395063e-08  1.10635083e-04  1.10635083e-04 ...  6.65395063e-08
  -2.68441974e-04  6.65395063e-08]
 [ 1.10635083e-04  2.59134889e-04  6.65395063e-08 ...  6.65395063e-08
   4.98686743e-04  1.10635083e-04]]
3.TKTKTKTK::::::: [[  1   3   4 ...   3   4   1]
 [  5   3   2 ...   5   4   4]
 [  3   5   2 ...   6   2   2]
 ...
 [484 485 485 ... 486 486 486]
 [486 485 485 ... 486 482 486]
 [485 483 486 ... 486 481 485]]
ncluster/block is 8
nhWeight/block is 200
!!!!!!!!!!!nBlock!!!!! see this !!!5
!!!!!!!!sparse ratio!!!!!!!! see this !!!6.0
blocked!!!
!!!!!!!wantto!!!!!!!!!! see this !!!0.25
!!!!!!!!!but!!!!!! see this !!!0.0183
1.TKTKTKTK::::::: [[-0.00805988 -0.00805988  0.09721942 ... -0.00805988 -0.00805988
   0.01591353]
 [ 0.09721942 -0.04617394  0.01591353 ... -0.04617394 -0.00805988
  -0.00805988]
 [ 0.01591353 -0.00805988 -0.00805988 ... -0.00805988 -0.00805988
  -0.00805988]
 ...
 [-0.06506478 -0.06506478 -0.06506478 ... -0.06506478 -0.06506478
   0.0387761 ]
 [-0.00596215 -0.00596215 -0.06506478 ... -0.00596215 -0.00596215
  -0.00596215]
 [-0.00596215 -0.00596215 -0.06506478 ... -0.00596215 -0.00596215
  -0.06506478]]
2.TKTKTKTK::::::: [[-0.00805988 -0.00805988  0.09721942 ... -0.00805988 -0.00805988
   0.01591353]
 [ 0.09721942 -0.04617394  0.01591353 ... -0.04617394 -0.00805988
  -0.00805988]
 [ 0.01591353 -0.00805988 -0.00805988 ... -0.00805988 -0.00805988
  -0.00805988]
 ...
 [-0.06506478 -0.06506478 -0.06506478 ... -0.06506478 -0.06506478
   0.0387761 ]
 [-0.00596215 -0.00596215 -0.06506478 ... -0.00596215 -0.00596215
  -0.00596215]
 [-0.00596215 -0.00596215 -0.06506478 ... -0.00596215 -0.00596215
  -0.06506478]]
3.TKTKTKTK::::::: [[ 4  4  5 ...  4  4  1]
 [ 5  3  1 ...  3  4  4]
 [ 1  4  4 ...  4  4  4]
 ...
 [34 34 34 ... 34 34 37]
 [38 38 34 ... 38 38 38]
 [38 38 34 ... 38 38 34]]
debug1.................................................add_forward__graph : end
debug2.................................................add_loss_graph : end
tk:grads_vars is  [(<tf.Tensor 'gradients/AddN_1:0' shape=(784, 1000) dtype=float32>, <tf.Variable 'dense1/weights:0' shape=(784, 1000) dtype=float32_ref>), (<tf.Tensor 'gradients/dense1/BiasAdd_grad/tuple/control_dependency_1:0' shape=(1000,) dtype=float32>, <tf.Variable 'dense1/biases:0' shape=(1000,) dtype=float32_ref>), (<tf.Tensor 'gradients/AddN:0' shape=(1000, 10) dtype=float32>, <tf.Variable 'dense2/weights:0' shape=(1000, 10) dtype=float32_ref>), (<tf.Tensor 'gradients/dense2/BiasAdd_grad/tuple/control_dependency_1:0' shape=(10,) dtype=float32>, <tf.Variable 'dense2/biases:0' shape=(10,) dtype=float32_ref>)]
0 trainable variable is !!!grad: Tensor("gradients/AddN_1:0", shape=(784, 1000), dtype=float32, device=/device:GPU:0)
0 trainable variable is !!!var: <tf.Variable 'dense1/weights:0' shape=(784, 1000) dtype=float32_ref>
1 trainable variable is !!!grad: Tensor("gradients/dense1/BiasAdd_grad/tuple/control_dependency_1:0", shape=(1000,), dtype=float32, device=/device:GPU:0)
1 trainable variable is !!!var: <tf.Variable 'dense1/biases:0' shape=(1000,) dtype=float32_ref>
2 trainable variable is !!!grad: Tensor("gradients/AddN:0", shape=(1000, 10), dtype=float32, device=/device:GPU:0)
2 trainable variable is !!!var: <tf.Variable 'dense2/weights:0' shape=(1000, 10) dtype=float32_ref>
3 trainable variable is !!!grad: Tensor("gradients/dense2/BiasAdd_grad/tuple/control_dependency_1:0", shape=(10,), dtype=float32, device=/device:GPU:0)
3 trainable variable is !!!var: <tf.Variable 'dense2/biases:0' shape=(10,) dtype=float32_ref>
debug3...............................................add_train_graph : end
Successfully Imported ../data/MNIST/train-images.gz
Extracting ../data/MNIST/train-images.gz
Successfully Imported ../data/MNIST/train-labels.gz
Extracting ../data/MNIST/train-labels.gz
Successfully Imported ../data/MNIST/test-images.gz
Extracting ../data/MNIST/test-images.gz
Successfully Imported ../data/MNIST/test-labels.gz
Extracting ../data/MNIST/test-labels.gz
Accuracy :  0.2574999988079071
-----------what is loss : 4.092518329620361
2019-11-12 15:06:12.493123: step 0, loss = 4.09 (1403.2 images/sec; 0.071 sec/batch)
-----------what is loss : 2.7914671897888184
2019-11-12 15:06:12.642958: step 10, loss = 2.79 (2803.2 images/sec; 0.036 sec/batch)
-----------what is loss : 1.977915644645691
2019-11-12 15:06:12.763769: step 20, loss = 1.98 (3261.9 images/sec; 0.031 sec/batch)
-----------what is loss : 1.2872576713562012
2019-11-12 15:06:12.884581: step 30, loss = 1.29 (3267.3 images/sec; 0.031 sec/batch)
-----------what is loss : 1.1097902059555054
2019-11-12 15:06:13.006400: step 40, loss = 1.11 (3210.3 images/sec; 0.031 sec/batch)
-----------what is loss : 1.1379528045654297
2019-11-12 15:06:13.128657: step 50, loss = 1.14 (3287.8 images/sec; 0.030 sec/batch)
-----------what is loss : 1.2282555103302002
2019-11-12 15:06:13.250015: step 60, loss = 1.23 (3281.9 images/sec; 0.030 sec/batch)
-----------what is loss : 0.9723881483078003
2019-11-12 15:06:13.370745: step 70, loss = 0.97 (3261.0 images/sec; 0.031 sec/batch)
-----------what is loss : 0.8897980451583862
2019-11-12 15:06:13.491836: step 80, loss = 0.89 (3279.8 images/sec; 0.030 sec/batch)
-----------what is loss : 1.0875298976898193
2019-11-12 15:06:13.613368: step 90, loss = 1.09 (3238.2 images/sec; 0.031 sec/batch)
-----------what is loss : 0.9678856134414673
2019-11-12 15:06:13.734367: step 100, loss = 0.97 (3290.7 images/sec; 0.030 sec/batch)
-----------what is loss : 0.9796839952468872
2019-11-12 15:06:13.855472: step 110, loss = 0.98 (3266.6 images/sec; 0.031 sec/batch)
-----------what is loss : 1.0146722793579102
2019-11-12 15:06:13.976122: step 120, loss = 1.01 (3270.6 images/sec; 0.031 sec/batch)
-----------what is loss : 1.193835973739624
2019-11-12 15:06:14.102400: step 130, loss = 1.19 (2783.6 images/sec; 0.036 sec/batch)
-----------what is loss : 1.0519212484359741
2019-11-12 15:06:14.225117: step 140, loss = 1.05 (3217.2 images/sec; 0.031 sec/batch)
-----------what is loss : 0.8739368915557861
2019-11-12 15:06:14.346958: step 150, loss = 0.87 (3277.9 images/sec; 0.031 sec/batch)
-----------what is loss : 0.9613229632377625
2019-11-12 15:06:14.467993: step 160, loss = 0.96 (3213.7 images/sec; 0.031 sec/batch)
-----------what is loss : 0.9342966079711914
2019-11-12 15:06:14.588807: step 170, loss = 0.93 (3204.7 images/sec; 0.031 sec/batch)
-----------what is loss : 0.8360162377357483
2019-11-12 15:06:14.709999: step 180, loss = 0.84 (3264.0 images/sec; 0.031 sec/batch)
-----------what is loss : 0.9090589880943298
2019-11-12 15:06:14.832408: step 190, loss = 0.91 (3179.5 images/sec; 0.031 sec/batch)
-----------what is loss : 0.8275126218795776
2019-11-12 15:06:14.954015: step 200, loss = 0.83 (3214.6 images/sec; 0.031 sec/batch)
-----------what is loss : 1.0944664478302002
2019-11-12 15:06:15.076205: step 210, loss = 1.09 (3250.6 images/sec; 0.031 sec/batch)
-----------what is loss : 0.9416118860244751
2019-11-12 15:06:15.198480: step 220, loss = 0.94 (3136.3 images/sec; 0.032 sec/batch)
-----------what is loss : 0.8013700246810913
2019-11-12 15:06:15.321064: step 230, loss = 0.80 (3201.2 images/sec; 0.031 sec/batch)
-----------what is loss : 0.9542235136032104
2019-11-12 15:06:15.441707: step 240, loss = 0.95 (3241.9 images/sec; 0.031 sec/batch)
-----------what is loss : 0.8418769240379333
2019-11-12 15:06:15.565648: step 250, loss = 0.84 (3171.2 images/sec; 0.032 sec/batch)
-----------what is loss : 0.8127403259277344
2019-11-12 15:06:15.753393: step 260, loss = 0.81 (1031.7 images/sec; 0.097 sec/batch)
-----------what is loss : 0.8324183821678162
2019-11-12 15:06:15.876875: step 270, loss = 0.83 (3202.0 images/sec; 0.031 sec/batch)
-----------what is loss : 0.8893826007843018
2019-11-12 15:06:15.999024: step 280, loss = 0.89 (3195.4 images/sec; 0.031 sec/batch)
-----------what is loss : 0.837883710861206
2019-11-12 15:06:16.121096: step 290, loss = 0.84 (3233.1 images/sec; 0.031 sec/batch)
-----------what is loss : 0.8487867116928101
2019-11-12 15:06:16.243281: step 300, loss = 0.85 (3215.5 images/sec; 0.031 sec/batch)
-----------what is loss : 0.8251542448997498
2019-11-12 15:06:16.365045: step 310, loss = 0.83 (3188.3 images/sec; 0.031 sec/batch)
-----------what is loss : 0.9888261556625366
2019-11-12 15:06:16.486348: step 320, loss = 0.99 (3242.5 images/sec; 0.031 sec/batch)
-----------what is loss : 0.8193805813789368
2019-11-12 15:06:16.608876: step 330, loss = 0.82 (3118.7 images/sec; 0.032 sec/batch)
-----------what is loss : 0.7363153100013733
2019-11-12 15:06:16.729939: step 340, loss = 0.74 (3186.0 images/sec; 0.031 sec/batch)
-----------what is loss : 0.7064067125320435
2019-11-12 15:06:16.852910: step 350, loss = 0.71 (3183.5 images/sec; 0.031 sec/batch)
-----------what is loss : 0.8587257862091064
2019-11-12 15:06:16.975470: step 360, loss = 0.86 (3081.1 images/sec; 0.032 sec/batch)
-----------what is loss : 1.0075345039367676
2019-11-12 15:06:17.098126: step 370, loss = 1.01 (3150.1 images/sec; 0.032 sec/batch)
-----------what is loss : 0.8783549070358276
2019-11-12 15:06:17.219693: step 380, loss = 0.88 (3219.1 images/sec; 0.031 sec/batch)
-----------what is loss : 0.9966142177581787
2019-11-12 15:06:17.341669: step 390, loss = 1.00 (3153.9 images/sec; 0.032 sec/batch)
-----------what is loss : 0.8318262696266174
2019-11-12 15:06:17.463249: step 400, loss = 0.83 (3181.4 images/sec; 0.031 sec/batch)
-----------what is loss : 0.8079900741577148
2019-11-12 15:06:17.584222: step 410, loss = 0.81 (3236.1 images/sec; 0.031 sec/batch)
-----------what is loss : 0.8752789497375488
2019-11-12 15:06:17.705803: step 420, loss = 0.88 (3211.6 images/sec; 0.031 sec/batch)
-----------what is loss : 0.9141680002212524
2019-11-12 15:06:17.827594: step 430, loss = 0.91 (3200.8 images/sec; 0.031 sec/batch)
-----------what is loss : 0.9419384598731995
2019-11-12 15:06:17.949594: step 440, loss = 0.94 (3206.0 images/sec; 0.031 sec/batch)
-----------what is loss : 0.8881680965423584
2019-11-12 15:06:18.070545: step 450, loss = 0.89 (3229.5 images/sec; 0.031 sec/batch)
-----------what is loss : 0.7466078400611877
2019-11-12 15:06:18.192476: step 460, loss = 0.75 (3194.8 images/sec; 0.031 sec/batch)
-----------what is loss : 0.8390683531761169
2019-11-12 15:06:18.314779: step 470, loss = 0.84 (3148.1 images/sec; 0.032 sec/batch)
-----------what is loss : 0.8526062369346619
2019-11-12 15:06:18.435910: step 480, loss = 0.85 (3182.7 images/sec; 0.031 sec/batch)
-----------what is loss : 0.8598333597183228
2019-11-12 15:06:18.557460: step 490, loss = 0.86 (3206.8 images/sec; 0.031 sec/batch)
-----------what is loss : 0.7029021978378296
2019-11-12 15:06:18.680478: step 500, loss = 0.70 (3107.0 images/sec; 0.032 sec/batch)
-----------what is loss : 0.8672173619270325
2019-11-12 15:06:18.807794: step 510, loss = 0.87 (2690.3 images/sec; 0.037 sec/batch)
-----------what is loss : 0.8733369708061218
2019-11-12 15:06:18.929661: step 520, loss = 0.87 (3168.0 images/sec; 0.032 sec/batch)
-----------what is loss : 0.8422494530677795
2019-11-12 15:06:19.106457: step 530, loss = 0.84 (3175.3 images/sec; 0.031 sec/batch)
-----------what is loss : 0.8242229223251343
2019-11-12 15:06:19.227805: step 540, loss = 0.82 (3232.0 images/sec; 0.031 sec/batch)
-----------what is loss : 0.6995452046394348
2019-11-12 15:06:19.349991: step 550, loss = 0.70 (3187.8 images/sec; 0.031 sec/batch)
-----------what is loss : 0.756342887878418
2019-11-12 15:06:19.472368: step 560, loss = 0.76 (3149.3 images/sec; 0.032 sec/batch)
-----------what is loss : 0.9516158103942871
2019-11-12 15:06:19.594497: step 570, loss = 0.95 (3205.3 images/sec; 0.031 sec/batch)
-----------what is loss : 0.81410151720047
2019-11-12 15:06:19.715942: step 580, loss = 0.81 (3221.6 images/sec; 0.031 sec/batch)
-----------what is loss : 0.7010852694511414
2019-11-12 15:06:19.838484: step 590, loss = 0.70 (3131.4 images/sec; 0.032 sec/batch)
-----------what is loss : 0.8133949637413025
2019-11-12 15:06:19.960977: step 600, loss = 0.81 (3161.7 images/sec; 0.032 sec/batch)
-----------what is loss : 0.6976165771484375
2019-11-12 15:06:20.083080: step 610, loss = 0.70 (3190.0 images/sec; 0.031 sec/batch)
-----------what is loss : 0.911049485206604
2019-11-12 15:06:20.204697: step 620, loss = 0.91 (3171.0 images/sec; 0.032 sec/batch)
-----------what is loss : 0.7835752964019775
2019-11-12 15:06:20.326916: step 630, loss = 0.78 (3172.2 images/sec; 0.032 sec/batch)
-----------what is loss : 0.8402945399284363
2019-11-12 15:06:20.448878: step 640, loss = 0.84 (3188.2 images/sec; 0.031 sec/batch)
-----------what is loss : 0.9334197044372559
2019-11-12 15:06:20.570976: step 650, loss = 0.93 (3201.7 images/sec; 0.031 sec/batch)
-----------what is loss : 0.6974179744720459
2019-11-12 15:06:20.697331: step 660, loss = 0.70 (2734.6 images/sec; 0.037 sec/batch)
-----------what is loss : 0.7592654228210449
2019-11-12 15:06:20.819390: step 670, loss = 0.76 (3217.0 images/sec; 0.031 sec/batch)
-----------what is loss : 0.7613204121589661
2019-11-12 15:06:20.940171: step 680, loss = 0.76 (3207.7 images/sec; 0.031 sec/batch)
-----------what is loss : 0.7988473176956177
2019-11-12 15:06:21.062757: step 690, loss = 0.80 (3114.5 images/sec; 0.032 sec/batch)
-----------what is loss : 0.8731380105018616
2019-11-12 15:06:21.184855: step 700, loss = 0.87 (3136.0 images/sec; 0.032 sec/batch)
-----------what is loss : 0.9029617309570312
2019-11-12 15:06:21.307745: step 710, loss = 0.90 (3156.2 images/sec; 0.032 sec/batch)
-----------what is loss : 0.7435133457183838
2019-11-12 15:06:21.429476: step 720, loss = 0.74 (3209.0 images/sec; 0.031 sec/batch)
-----------what is loss : 0.931159257888794
2019-11-12 15:06:21.551415: step 730, loss = 0.93 (3139.4 images/sec; 0.032 sec/batch)
-----------what is loss : 0.8960130214691162
2019-11-12 15:06:21.673190: step 740, loss = 0.90 (3177.0 images/sec; 0.031 sec/batch)
-----------what is loss : 0.6882390975952148
2019-11-12 15:06:21.800384: step 750, loss = 0.69 (2697.2 images/sec; 0.037 sec/batch)
-----------what is loss : 0.8435714244842529
2019-11-12 15:06:21.921927: step 760, loss = 0.84 (3208.1 images/sec; 0.031 sec/batch)
-----------what is loss : 0.9519023895263672
2019-11-12 15:06:22.044246: step 770, loss = 0.95 (3170.3 images/sec; 0.032 sec/batch)
-----------what is loss : 0.753618061542511
2019-11-12 15:06:22.165055: step 780, loss = 0.75 (3179.8 images/sec; 0.031 sec/batch)
-----------what is loss : 0.8305473327636719
2019-11-12 15:06:22.342410: step 790, loss = 0.83 (3195.4 images/sec; 0.031 sec/batch)
-----------what is loss : 0.857025146484375
2019-11-12 15:06:22.463909: step 800, loss = 0.86 (3209.2 images/sec; 0.031 sec/batch)
-----------what is loss : 0.7566837072372437
2019-11-12 15:06:22.585278: step 810, loss = 0.76 (3213.2 images/sec; 0.031 sec/batch)
-----------what is loss : 0.7154556512832642
2019-11-12 15:06:22.710453: step 820, loss = 0.72 (3141.6 images/sec; 0.032 sec/batch)
-----------what is loss : 0.8177588582038879
2019-11-12 15:06:22.835349: step 830, loss = 0.82 (3134.1 images/sec; 0.032 sec/batch)
-----------what is loss : 0.5992860198020935
2019-11-12 15:06:22.963675: step 840, loss = 0.60 (3157.2 images/sec; 0.032 sec/batch)
-----------what is loss : 0.6994848251342773
2019-11-12 15:06:23.090140: step 850, loss = 0.70 (3125.1 images/sec; 0.032 sec/batch)
-----------what is loss : 0.6379087567329407
2019-11-12 15:06:23.218321: step 860, loss = 0.64 (3106.3 images/sec; 0.032 sec/batch)
-----------what is loss : 0.7964023351669312
2019-11-12 15:06:23.343903: step 870, loss = 0.80 (3160.4 images/sec; 0.032 sec/batch)
-----------what is loss : 0.7093187570571899
2019-11-12 15:06:23.469180: step 880, loss = 0.71 (3120.7 images/sec; 0.032 sec/batch)
-----------what is loss : 0.6446748375892639
2019-11-12 15:06:23.594820: step 890, loss = 0.64 (3123.6 images/sec; 0.032 sec/batch)
-----------what is loss : 0.7332605719566345
2019-11-12 15:06:23.726653: step 900, loss = 0.73 (2674.0 images/sec; 0.037 sec/batch)
-----------what is loss : 0.8444764614105225
2019-11-12 15:06:23.852137: step 910, loss = 0.84 (3149.3 images/sec; 0.032 sec/batch)
-----------what is loss : 0.8835785388946533
2019-11-12 15:06:23.977700: step 920, loss = 0.88 (3175.6 images/sec; 0.031 sec/batch)
-----------what is loss : 0.8581238389015198
2019-11-12 15:06:24.108994: step 930, loss = 0.86 (3067.5 images/sec; 0.033 sec/batch)
-----------what is loss : 0.8304214477539062
2019-11-12 15:06:24.236104: step 940, loss = 0.83 (3090.9 images/sec; 0.032 sec/batch)
-----------what is loss : 0.8864328265190125
2019-11-12 15:06:24.362581: step 950, loss = 0.89 (3173.2 images/sec; 0.032 sec/batch)
-----------what is loss : 0.747649073600769
2019-11-12 15:06:24.486501: step 960, loss = 0.75 (3127.9 images/sec; 0.032 sec/batch)
-----------what is loss : 0.6790411472320557
2019-11-12 15:06:24.610314: step 970, loss = 0.68 (3181.8 images/sec; 0.031 sec/batch)
-----------what is loss : 0.7819294929504395
2019-11-12 15:06:24.733276: step 980, loss = 0.78 (3157.5 images/sec; 0.032 sec/batch)
-----------what is loss : 0.7361406087875366
2019-11-12 15:06:24.854901: step 990, loss = 0.74 (3190.2 images/sec; 0.031 sec/batch)
optimization Finished
Accuracy :  0.8651000010967255
tuple_tem key is dense2
tuple_tem key is dense1
tuple_tem value is (1000, 10)
tuple_tem value is (10,)
tuple_tem value is (784, 1000)
tuple_tem value is (1000,)
tk is  [[-1.2790030e-04 -2.4263334e-05  3.2459564e-05 ... -2.4263334e-05
   3.2459564e-05 -1.2790030e-04]
 [ 8.5770291e-05 -2.4263334e-05 -7.1314767e-05 ...  8.5770291e-05
   3.2459564e-05  3.2459564e-05]
 [-2.4263334e-05  8.5770291e-05 -7.1314767e-05 ...  1.6076250e-04
  -7.1314767e-05 -7.1314767e-05]
 ...
 [-6.2351130e-05  6.0840663e-05  6.0840663e-05 ...  3.6591530e-08
   3.6591530e-08  3.6591530e-08]
 [ 3.6591530e-08  6.0840663e-05  6.0840663e-05 ...  3.6591530e-08
  -1.4762227e-04  3.6591530e-08]
 [ 6.0840663e-05  1.4250394e-04  3.6591530e-08 ...  3.6591530e-08
   2.7423899e-04  6.0840663e-05]]
tk is  [[  1   3   4 ...   3   4   1]
 [  5   3   2 ...   5   4   4]
 [  3   5   2 ...   6   2   2]
 ...
 [484 485 485 ... 486 486 486]
 [486 485 485 ... 486 482 486]
 [485 483 486 ... 486 481 485]]
tk is  [[-0.0219659  -0.0219659   0.17626409 ... -0.0219659  -0.0219659
   0.0400562 ]
 [ 0.17626409 -0.07454456  0.0400562  ... -0.07454456 -0.0219659
  -0.0219659 ]
 [ 0.0400562  -0.0219659  -0.0219659  ... -0.0219659  -0.0219659
  -0.0219659 ]
 ...
 [-0.15087907 -0.15087907 -0.15087907 ... -0.15087907 -0.15087907
   0.12422496]
 [-0.02072335 -0.02072335 -0.15087907 ... -0.02072335 -0.02072335
  -0.02072335]
 [-0.02072335 -0.02072335 -0.15087907 ... -0.02072335 -0.02072335
  -0.15087907]]
tk is  [[ 4  4  5 ...  4  4  1]
 [ 5  3  1 ...  3  4  4]
 [ 1  4  4 ...  4  4  4]
 ...
 [34 34 34 ... 34 34 37]
 [38 38 34 ... 38 38 38]
 [38 38 34 ... 38 38 34]]
tk2 is  True
tk2 is  5
=============================================0.75===========================================
mc.LOAD_PRETRAINED_MODEL is True
OK to save ./MNIST/train/MNIST_PRUNING_INFO.pkl
ncluster/block is 8
nhWeight/block is 13
!!!!!!!!!!!nBlock!!!!! see this !!!61
!!!!!!!!sparse ratio!!!!!!!! see this !!!6.0
blocked!!!
!!!!!!!wantto!!!!!!!!!! see this !!!0.25
!!!!!!!!!but!!!!!! see this !!!0.09202933673469388
1.TKTKTKTK::::::: [[-1.2790030e-04 -2.4263334e-05  3.2459564e-05 ... -2.4263334e-05
   3.2459564e-05 -1.2790030e-04]
 [ 8.5770291e-05 -2.4263334e-05 -7.1314767e-05 ...  8.5770291e-05
   3.2459564e-05  3.2459564e-05]
 [-2.4263334e-05  8.5770291e-05 -7.1314767e-05 ...  1.6076250e-04
  -7.1314767e-05 -7.1314767e-05]
 ...
 [-6.2351130e-05  6.0840663e-05  6.0840663e-05 ...  3.6591530e-08
   3.6591530e-08  3.6591530e-08]
 [ 3.6591530e-08  6.0840663e-05  6.0840663e-05 ...  3.6591530e-08
  -1.4762227e-04  3.6591530e-08]
 [ 6.0840663e-05  1.4250394e-04  3.6591530e-08 ...  3.6591530e-08
   2.7423899e-04  6.0840663e-05]]
2.TKTKTKTK::::::: [[-0.00000000e+00 -2.42633341e-05  3.24595640e-05 ... -2.42633341e-05
   3.24595640e-05 -0.00000000e+00]
 [ 8.57702908e-05 -2.42633341e-05 -7.13147674e-05 ...  8.57702908e-05
   3.24595640e-05  3.24595640e-05]
 [-2.42633341e-05  8.57702908e-05 -7.13147674e-05 ...  0.00000000e+00
  -7.13147674e-05 -7.13147674e-05]
 ...
 [-6.23511296e-05  6.08406626e-05  6.08406626e-05 ...  3.65915298e-08
   3.65915298e-08  3.65915298e-08]
 [ 3.65915298e-08  6.08406626e-05  6.08406626e-05 ...  3.65915298e-08
  -1.47622268e-04  3.65915298e-08]
 [ 6.08406626e-05  1.42503937e-04  3.65915298e-08 ...  3.65915298e-08
   0.00000000e+00  6.08406626e-05]]
3.TKTKTKTK::::::: [[  1   3   4 ...   3   4   1]
 [  5   3   2 ...   5   4   4]
 [  3   5   2 ...   6   2   2]
 ...
 [484 485 485 ... 486 486 486]
 [486 485 485 ... 486 482 486]
 [485 483 486 ... 486 481 485]]
ncluster/block is 8
nhWeight/block is 200
!!!!!!!!!!!nBlock!!!!! see this !!!5
!!!!!!!!sparse ratio!!!!!!!! see this !!!6.0
blocked!!!
!!!!!!!wantto!!!!!!!!!! see this !!!0.25
!!!!!!!!!but!!!!!! see this !!!0.0618
1.TKTKTKTK::::::: [[-0.0219659  -0.0219659   0.17626409 ... -0.0219659  -0.0219659
   0.0400562 ]
 [ 0.17626409 -0.07454456  0.0400562  ... -0.07454456 -0.0219659
  -0.0219659 ]
 [ 0.0400562  -0.0219659  -0.0219659  ... -0.0219659  -0.0219659
  -0.0219659 ]
 ...
 [-0.15087907 -0.15087907 -0.15087907 ... -0.15087907 -0.15087907
   0.12422496]
 [-0.02072335 -0.02072335 -0.15087907 ... -0.02072335 -0.02072335
  -0.02072335]
 [-0.02072335 -0.02072335 -0.15087907 ... -0.02072335 -0.02072335
  -0.15087907]]
2.TKTKTKTK::::::: [[-0.0219659  -0.0219659   0.17626409 ... -0.0219659  -0.0219659
   0.0400562 ]
 [ 0.17626409 -0.07454456  0.0400562  ... -0.07454456 -0.0219659
  -0.0219659 ]
 [ 0.0400562  -0.0219659  -0.0219659  ... -0.0219659  -0.0219659
  -0.0219659 ]
 ...
 [-0.15087907 -0.15087907 -0.15087907 ... -0.15087907 -0.15087907
   0.12422496]
 [-0.02072335 -0.02072335 -0.15087907 ... -0.02072335 -0.02072335
  -0.02072335]
 [-0.02072335 -0.02072335 -0.15087907 ... -0.02072335 -0.02072335
  -0.15087907]]
3.TKTKTKTK::::::: [[ 4  4  5 ...  4  4  1]
 [ 5  3  1 ...  3  4  4]
 [ 1  4  4 ...  4  4  4]
 ...
 [34 34 34 ... 34 34 37]
 [38 38 34 ... 38 38 38]
 [38 38 34 ... 38 38 34]]
debug1.................................................add_forward__graph : end
debug2.................................................add_loss_graph : end
tk:grads_vars is  [(<tf.Tensor 'gradients/AddN_1:0' shape=(784, 1000) dtype=float32>, <tf.Variable 'dense1/weights:0' shape=(784, 1000) dtype=float32_ref>), (<tf.Tensor 'gradients/dense1/BiasAdd_grad/tuple/control_dependency_1:0' shape=(1000,) dtype=float32>, <tf.Variable 'dense1/biases:0' shape=(1000,) dtype=float32_ref>), (<tf.Tensor 'gradients/AddN:0' shape=(1000, 10) dtype=float32>, <tf.Variable 'dense2/weights:0' shape=(1000, 10) dtype=float32_ref>), (<tf.Tensor 'gradients/dense2/BiasAdd_grad/tuple/control_dependency_1:0' shape=(10,) dtype=float32>, <tf.Variable 'dense2/biases:0' shape=(10,) dtype=float32_ref>)]
0 trainable variable is !!!grad: Tensor("gradients/AddN_1:0", shape=(784, 1000), dtype=float32, device=/device:GPU:0)
0 trainable variable is !!!var: <tf.Variable 'dense1/weights:0' shape=(784, 1000) dtype=float32_ref>
1 trainable variable is !!!grad: Tensor("gradients/dense1/BiasAdd_grad/tuple/control_dependency_1:0", shape=(1000,), dtype=float32, device=/device:GPU:0)
1 trainable variable is !!!var: <tf.Variable 'dense1/biases:0' shape=(1000,) dtype=float32_ref>
2 trainable variable is !!!grad: Tensor("gradients/AddN:0", shape=(1000, 10), dtype=float32, device=/device:GPU:0)
2 trainable variable is !!!var: <tf.Variable 'dense2/weights:0' shape=(1000, 10) dtype=float32_ref>
3 trainable variable is !!!grad: Tensor("gradients/dense2/BiasAdd_grad/tuple/control_dependency_1:0", shape=(10,), dtype=float32, device=/device:GPU:0)
3 trainable variable is !!!var: <tf.Variable 'dense2/biases:0' shape=(10,) dtype=float32_ref>
debug3...............................................add_train_graph : end
Successfully Imported ../data/MNIST/train-images.gz
Extracting ../data/MNIST/train-images.gz
Successfully Imported ../data/MNIST/train-labels.gz
Extracting ../data/MNIST/train-labels.gz
Successfully Imported ../data/MNIST/test-images.gz
Extracting ../data/MNIST/test-images.gz
Successfully Imported ../data/MNIST/test-labels.gz
Extracting ../data/MNIST/test-labels.gz
Accuracy :  0.23719999924302101
-----------what is loss : 3.4698221683502197
2019-11-12 15:06:29.394936: step 0, loss = 3.47 (1350.3 images/sec; 0.074 sec/batch)
-----------what is loss : 2.365264892578125
2019-11-12 15:06:29.539085: step 10, loss = 2.37 (3217.1 images/sec; 0.031 sec/batch)
-----------what is loss : 1.9384784698486328
2019-11-12 15:06:29.659354: step 20, loss = 1.94 (3149.8 images/sec; 0.032 sec/batch)
-----------what is loss : 1.5283827781677246
2019-11-12 15:06:29.778793: step 30, loss = 1.53 (3234.9 images/sec; 0.031 sec/batch)
-----------what is loss : 1.4295730590820312
2019-11-12 15:06:29.898037: step 40, loss = 1.43 (3214.6 images/sec; 0.031 sec/batch)
-----------what is loss : 1.3661552667617798
2019-11-12 15:06:30.019014: step 50, loss = 1.37 (3253.6 images/sec; 0.031 sec/batch)
-----------what is loss : 1.4393161535263062
2019-11-12 15:06:30.139465: step 60, loss = 1.44 (3184.9 images/sec; 0.031 sec/batch)
-----------what is loss : 1.103632926940918
2019-11-12 15:06:30.259712: step 70, loss = 1.10 (3135.2 images/sec; 0.032 sec/batch)
-----------what is loss : 1.1397161483764648
2019-11-12 15:06:30.380061: step 80, loss = 1.14 (3236.7 images/sec; 0.031 sec/batch)
-----------what is loss : 1.1871553659439087
2019-11-12 15:06:30.499373: step 90, loss = 1.19 (3246.6 images/sec; 0.031 sec/batch)
-----------what is loss : 0.9637255668640137
2019-11-12 15:06:30.618775: step 100, loss = 0.96 (3216.5 images/sec; 0.031 sec/batch)
-----------what is loss : 1.1108380556106567
2019-11-12 15:06:30.738531: step 110, loss = 1.11 (3234.3 images/sec; 0.031 sec/batch)
-----------what is loss : 1.0334391593933105
2019-11-12 15:06:30.858038: step 120, loss = 1.03 (3223.4 images/sec; 0.031 sec/batch)
-----------what is loss : 1.1374642848968506
2019-11-12 15:06:30.977694: step 130, loss = 1.14 (3246.1 images/sec; 0.031 sec/batch)
-----------what is loss : 1.0214605331420898
2019-11-12 15:06:31.097567: step 140, loss = 1.02 (3187.9 images/sec; 0.031 sec/batch)
-----------what is loss : 0.8827165365219116
2019-11-12 15:06:31.216556: step 150, loss = 0.88 (3265.4 images/sec; 0.031 sec/batch)
-----------what is loss : 1.0257714986801147
2019-11-12 15:06:31.336409: step 160, loss = 1.03 (3195.1 images/sec; 0.031 sec/batch)
-----------what is loss : 0.9448661804199219
2019-11-12 15:06:31.456367: step 170, loss = 0.94 (3199.0 images/sec; 0.031 sec/batch)
-----------what is loss : 0.9343130588531494
2019-11-12 15:06:31.576039: step 180, loss = 0.93 (3204.6 images/sec; 0.031 sec/batch)
-----------what is loss : 0.9164917469024658
2019-11-12 15:06:31.697698: step 190, loss = 0.92 (3177.0 images/sec; 0.031 sec/batch)
-----------what is loss : 0.8062436580657959
2019-11-12 15:06:31.824510: step 200, loss = 0.81 (2710.2 images/sec; 0.037 sec/batch)
-----------what is loss : 0.9879013299942017
2019-11-12 15:06:31.944372: step 210, loss = 0.99 (3222.8 images/sec; 0.031 sec/batch)
-----------what is loss : 0.9745800495147705
2019-11-12 15:06:32.065999: step 220, loss = 0.97 (3138.5 images/sec; 0.032 sec/batch)
-----------what is loss : 0.7702264785766602
2019-11-12 15:06:32.188252: step 230, loss = 0.77 (3144.8 images/sec; 0.032 sec/batch)
-----------what is loss : 0.8714361786842346
2019-11-12 15:06:32.314557: step 240, loss = 0.87 (2822.9 images/sec; 0.035 sec/batch)
-----------what is loss : 0.895067036151886
2019-11-12 15:06:32.436270: step 250, loss = 0.90 (3159.4 images/sec; 0.032 sec/batch)
-----------what is loss : 0.7690343856811523
2019-11-12 15:06:32.622209: step 260, loss = 0.77 (1033.0 images/sec; 0.097 sec/batch)
-----------what is loss : 0.9598351716995239
2019-11-12 15:06:32.743021: step 270, loss = 0.96 (3178.9 images/sec; 0.031 sec/batch)
-----------what is loss : 0.8675492405891418
2019-11-12 15:06:32.863349: step 280, loss = 0.87 (3167.0 images/sec; 0.032 sec/batch)
-----------what is loss : 0.9605877995491028
2019-11-12 15:06:32.984347: step 290, loss = 0.96 (3138.7 images/sec; 0.032 sec/batch)
-----------what is loss : 0.8553546071052551
2019-11-12 15:06:33.106673: step 300, loss = 0.86 (3191.2 images/sec; 0.031 sec/batch)
-----------what is loss : 0.8016092777252197
2019-11-12 15:06:33.227379: step 310, loss = 0.80 (3217.0 images/sec; 0.031 sec/batch)
-----------what is loss : 0.839858889579773
2019-11-12 15:06:33.347545: step 320, loss = 0.84 (3194.6 images/sec; 0.031 sec/batch)
-----------what is loss : 0.8711452484130859
2019-11-12 15:06:33.474578: step 330, loss = 0.87 (2713.6 images/sec; 0.037 sec/batch)
-----------what is loss : 0.6463233232498169
2019-11-12 15:06:33.594932: step 340, loss = 0.65 (3170.2 images/sec; 0.032 sec/batch)
-----------what is loss : 0.8239961862564087
2019-11-12 15:06:33.717348: step 350, loss = 0.82 (3196.5 images/sec; 0.031 sec/batch)
-----------what is loss : 0.6357269883155823
2019-11-12 15:06:33.838101: step 360, loss = 0.64 (3122.6 images/sec; 0.032 sec/batch)
-----------what is loss : 0.9414260387420654
2019-11-12 15:06:33.958272: step 370, loss = 0.94 (3163.4 images/sec; 0.032 sec/batch)
-----------what is loss : 0.9260565638542175
2019-11-12 15:06:34.078687: step 380, loss = 0.93 (3111.1 images/sec; 0.032 sec/batch)
-----------what is loss : 0.7414156198501587
2019-11-12 15:06:34.198995: step 390, loss = 0.74 (3223.8 images/sec; 0.031 sec/batch)
-----------what is loss : 0.8379592299461365
2019-11-12 15:06:34.321812: step 400, loss = 0.84 (3155.4 images/sec; 0.032 sec/batch)
-----------what is loss : 0.6901742219924927
2019-11-12 15:06:34.442446: step 410, loss = 0.69 (3207.9 images/sec; 0.031 sec/batch)
-----------what is loss : 0.7677499651908875
2019-11-12 15:06:34.563655: step 420, loss = 0.77 (3191.9 images/sec; 0.031 sec/batch)
-----------what is loss : 0.8144890069961548
2019-11-12 15:06:34.684816: step 430, loss = 0.81 (3146.8 images/sec; 0.032 sec/batch)
-----------what is loss : 0.6743654012680054
2019-11-12 15:06:34.806673: step 440, loss = 0.67 (3153.7 images/sec; 0.032 sec/batch)
-----------what is loss : 0.767244279384613
2019-11-12 15:06:34.932993: step 450, loss = 0.77 (2733.3 images/sec; 0.037 sec/batch)
-----------what is loss : 0.8555477857589722
2019-11-12 15:06:35.054119: step 460, loss = 0.86 (3170.2 images/sec; 0.032 sec/batch)
-----------what is loss : 0.8293355703353882
2019-11-12 15:06:35.175461: step 470, loss = 0.83 (3096.1 images/sec; 0.032 sec/batch)
-----------what is loss : 0.7723582983016968
2019-11-12 15:06:35.301149: step 480, loss = 0.77 (3045.1 images/sec; 0.033 sec/batch)
-----------what is loss : 0.7129029631614685
2019-11-12 15:06:35.421335: step 490, loss = 0.71 (3167.3 images/sec; 0.032 sec/batch)
-----------what is loss : 0.7626368999481201
2019-11-12 15:06:35.542126: step 500, loss = 0.76 (3166.3 images/sec; 0.032 sec/batch)
-----------what is loss : 0.8321070671081543
2019-11-12 15:06:35.662980: step 510, loss = 0.83 (3166.1 images/sec; 0.032 sec/batch)
-----------what is loss : 0.8183553814888
2019-11-12 15:06:35.782918: step 520, loss = 0.82 (3173.6 images/sec; 0.032 sec/batch)
-----------what is loss : 0.7041233777999878
2019-11-12 15:06:35.959782: step 530, loss = 0.70 (3200.0 images/sec; 0.031 sec/batch)
-----------what is loss : 0.9240384697914124
2019-11-12 15:06:36.081734: step 540, loss = 0.92 (3161.1 images/sec; 0.032 sec/batch)
-----------what is loss : 0.6186521053314209
2019-11-12 15:06:36.201287: step 550, loss = 0.62 (3168.9 images/sec; 0.032 sec/batch)
-----------what is loss : 0.7735632658004761
2019-11-12 15:06:36.322420: step 560, loss = 0.77 (3194.1 images/sec; 0.031 sec/batch)
-----------what is loss : 0.8404754400253296
2019-11-12 15:06:36.444006: step 570, loss = 0.84 (3133.0 images/sec; 0.032 sec/batch)
-----------what is loss : 0.73455810546875
2019-11-12 15:06:36.565525: step 580, loss = 0.73 (3123.4 images/sec; 0.032 sec/batch)
-----------what is loss : 0.6954483985900879
2019-11-12 15:06:36.686005: step 590, loss = 0.70 (3143.7 images/sec; 0.032 sec/batch)
-----------what is loss : 0.6866719722747803
2019-11-12 15:06:36.810716: step 600, loss = 0.69 (2716.9 images/sec; 0.037 sec/batch)
-----------what is loss : 0.7746634483337402
2019-11-12 15:06:36.929471: step 610, loss = 0.77 (3229.0 images/sec; 0.031 sec/batch)
-----------what is loss : 0.7158149480819702
2019-11-12 15:06:37.050319: step 620, loss = 0.72 (3180.8 images/sec; 0.031 sec/batch)
-----------what is loss : 0.6829882860183716
2019-11-12 15:06:37.170032: step 630, loss = 0.68 (3247.7 images/sec; 0.031 sec/batch)
-----------what is loss : 0.758750855922699
2019-11-12 15:06:37.291041: step 640, loss = 0.76 (3163.9 images/sec; 0.032 sec/batch)
-----------what is loss : 0.8284934759140015
2019-11-12 15:06:37.412324: step 650, loss = 0.83 (3141.0 images/sec; 0.032 sec/batch)
-----------what is loss : 0.7048466205596924
2019-11-12 15:06:37.531117: step 660, loss = 0.70 (3204.9 images/sec; 0.031 sec/batch)
-----------what is loss : 0.7132250666618347
2019-11-12 15:06:37.650791: step 670, loss = 0.71 (3180.2 images/sec; 0.031 sec/batch)
-----------what is loss : 0.759695291519165
2019-11-12 15:06:37.770973: step 680, loss = 0.76 (3199.2 images/sec; 0.031 sec/batch)
-----------what is loss : 0.6708448529243469
2019-11-12 15:06:37.898112: step 690, loss = 0.67 (2696.7 images/sec; 0.037 sec/batch)
-----------what is loss : 0.6168594360351562
2019-11-12 15:06:38.018861: step 700, loss = 0.62 (3166.4 images/sec; 0.032 sec/batch)
-----------what is loss : 0.732597827911377
2019-11-12 15:06:38.138967: step 710, loss = 0.73 (3153.1 images/sec; 0.032 sec/batch)
-----------what is loss : 0.7224774360656738
2019-11-12 15:06:38.260472: step 720, loss = 0.72 (3120.2 images/sec; 0.032 sec/batch)
-----------what is loss : 0.6823031902313232
2019-11-12 15:06:38.382682: step 730, loss = 0.68 (3169.2 images/sec; 0.032 sec/batch)
-----------what is loss : 0.8241686820983887
2019-11-12 15:06:38.502670: step 740, loss = 0.82 (3207.8 images/sec; 0.031 sec/batch)
-----------what is loss : 0.8328691124916077
2019-11-12 15:06:38.627516: step 750, loss = 0.83 (2738.2 images/sec; 0.037 sec/batch)
-----------what is loss : 0.7221057415008545
2019-11-12 15:06:38.747079: step 760, loss = 0.72 (3203.0 images/sec; 0.031 sec/batch)
-----------what is loss : 0.8121660351753235
2019-11-12 15:06:38.867705: step 770, loss = 0.81 (3113.8 images/sec; 0.032 sec/batch)
-----------what is loss : 0.7817119359970093
2019-11-12 15:06:38.987064: step 780, loss = 0.78 (3163.3 images/sec; 0.032 sec/batch)
-----------what is loss : 0.7504011392593384
2019-11-12 15:06:39.163588: step 790, loss = 0.75 (3220.3 images/sec; 0.031 sec/batch)
-----------what is loss : 0.6569012403488159
2019-11-12 15:06:39.284185: step 800, loss = 0.66 (3214.3 images/sec; 0.031 sec/batch)
-----------what is loss : 0.6672077775001526
2019-11-12 15:06:39.404806: step 810, loss = 0.67 (3140.9 images/sec; 0.032 sec/batch)
-----------what is loss : 0.692045271396637
2019-11-12 15:06:39.525685: step 820, loss = 0.69 (3185.3 images/sec; 0.031 sec/batch)
-----------what is loss : 0.8826836943626404
2019-11-12 15:06:39.646399: step 830, loss = 0.88 (3171.2 images/sec; 0.032 sec/batch)
-----------what is loss : 0.9306344985961914
2019-11-12 15:06:39.767921: step 840, loss = 0.93 (3165.7 images/sec; 0.032 sec/batch)
-----------what is loss : 0.6819298267364502
2019-11-12 15:06:39.888228: step 850, loss = 0.68 (3196.4 images/sec; 0.031 sec/batch)
-----------what is loss : 0.6370489597320557
2019-11-12 15:06:40.008131: step 860, loss = 0.64 (3147.7 images/sec; 0.032 sec/batch)
-----------what is loss : 0.6736651659011841
2019-11-12 15:06:40.128146: step 870, loss = 0.67 (3172.8 images/sec; 0.032 sec/batch)
-----------what is loss : 0.6070206761360168
2019-11-12 15:06:40.253379: step 880, loss = 0.61 (2728.1 images/sec; 0.037 sec/batch)
-----------what is loss : 0.8005462288856506
2019-11-12 15:06:40.373550: step 890, loss = 0.80 (3145.3 images/sec; 0.032 sec/batch)
-----------what is loss : 0.7178125977516174
2019-11-12 15:06:40.494915: step 900, loss = 0.72 (3088.6 images/sec; 0.032 sec/batch)
-----------what is loss : 0.8609920144081116
2019-11-12 15:06:40.614992: step 910, loss = 0.86 (3201.1 images/sec; 0.031 sec/batch)
-----------what is loss : 0.7314481735229492
2019-11-12 15:06:40.736871: step 920, loss = 0.73 (3142.0 images/sec; 0.032 sec/batch)
-----------what is loss : 0.5868058204650879
2019-11-12 15:06:40.856886: step 930, loss = 0.59 (3211.4 images/sec; 0.031 sec/batch)
-----------what is loss : 0.6851357221603394
2019-11-12 15:06:40.976655: step 940, loss = 0.69 (3179.1 images/sec; 0.031 sec/batch)
-----------what is loss : 0.7249279022216797
2019-11-12 15:06:41.097575: step 950, loss = 0.72 (3147.5 images/sec; 0.032 sec/batch)
-----------what is loss : 0.6291911005973816
2019-11-12 15:06:41.217135: step 960, loss = 0.63 (3175.6 images/sec; 0.031 sec/batch)
-----------what is loss : 0.7517659664154053
2019-11-12 15:06:41.339144: step 970, loss = 0.75 (3141.0 images/sec; 0.032 sec/batch)
-----------what is loss : 0.6809756755828857
2019-11-12 15:06:41.459607: step 980, loss = 0.68 (3185.3 images/sec; 0.031 sec/batch)
-----------what is loss : 0.6411838531494141
2019-11-12 15:06:41.580813: step 990, loss = 0.64 (3139.3 images/sec; 0.032 sec/batch)
optimization Finished
Accuracy :  0.8850000017881393
tuple_tem key is dense2
tuple_tem key is dense1
tuple_tem value is (1000, 10)
tuple_tem value is (10,)
tuple_tem value is (784, 1000)
tuple_tem value is (1000,)
tk is  [[-0.0000000e+00 -1.2924851e-05  1.9805873e-05 ... -1.2924851e-05
   1.9805873e-05 -0.0000000e+00]
 [ 4.8933478e-05 -1.2924851e-05 -3.7172504e-05 ...  4.8933478e-05
   1.9805873e-05  1.9805873e-05]
 [-1.2924851e-05  4.8933478e-05 -3.7172504e-05 ...  0.0000000e+00
  -3.7172504e-05 -3.7172504e-05]
 ...
 [-3.4288281e-05  3.3457618e-05  3.3457618e-05 ...  2.0122480e-08
   2.0122480e-08  2.0122480e-08]
 [ 2.0122480e-08  3.3457618e-05  3.3457618e-05 ...  2.0122480e-08
  -8.1180777e-05  2.0122480e-08]
 [ 3.3457618e-05  7.8365978e-05  2.0122480e-08 ...  2.0122480e-08
   0.0000000e+00  3.3457618e-05]]
tk is  [[  1   3   4 ...   3   4   1]
 [  5   3   2 ...   5   4   4]
 [  3   5   2 ...   6   2   2]
 ...
 [484 485 485 ... 486 486 486]
 [486 485 485 ... 486 482 486]
 [485 483 486 ... 486 481 485]]
tk is  [[-0.0389565  -0.0389565   0.26544023 ... -0.0389565  -0.0389565
   0.07410996]
 [ 0.26544023 -0.11477213  0.07410996 ... -0.11477213 -0.0389565
  -0.0389565 ]
 [ 0.07410996 -0.0389565  -0.0389565  ... -0.0389565  -0.0389565
  -0.0389565 ]
 ...
 [-0.16967633 -0.16967633 -0.16967633 ... -0.16967633 -0.16967633
   0.14513288]
 [-0.01289844 -0.01289844 -0.16967633 ... -0.01289844 -0.01289844
  -0.01289844]
 [-0.01289844 -0.01289844 -0.16967633 ... -0.01289844 -0.01289844
  -0.16967633]]
tk is  [[ 4  4  5 ...  4  4  1]
 [ 5  3  1 ...  3  4  4]
 [ 1  4  4 ...  4  4  4]
 ...
 [34 34 34 ... 34 34 37]
 [38 38 34 ... 38 38 38]
 [38 38 34 ... 38 38 34]]
tk2 is  True
tk2 is  5
=============================================0.75===========================================
mc.LOAD_PRETRAINED_MODEL is True
OK to save ./MNIST/train/MNIST_PRUNING_INFO.pkl
ncluster/block is 8
nhWeight/block is 13
!!!!!!!!!!!nBlock!!!!! see this !!!61
!!!!!!!!sparse ratio!!!!!!!! see this !!!6.0
blocked!!!
!!!!!!!wantto!!!!!!!!!! see this !!!0.25
!!!!!!!!!but!!!!!! see this !!!0.13791454081632654
1.TKTKTKTK::::::: [[-0.0000000e+00 -1.2924851e-05  1.9805873e-05 ... -1.2924851e-05
   1.9805873e-05 -0.0000000e+00]
 [ 4.8933478e-05 -1.2924851e-05 -3.7172504e-05 ...  4.8933478e-05
   1.9805873e-05  1.9805873e-05]
 [-1.2924851e-05  4.8933478e-05 -3.7172504e-05 ...  0.0000000e+00
  -3.7172504e-05 -3.7172504e-05]
 ...
 [-3.4288281e-05  3.3457618e-05  3.3457618e-05 ...  2.0122480e-08
   2.0122480e-08  2.0122480e-08]
 [ 2.0122480e-08  3.3457618e-05  3.3457618e-05 ...  2.0122480e-08
  -8.1180777e-05  2.0122480e-08]
 [ 3.3457618e-05  7.8365978e-05  2.0122480e-08 ...  2.0122480e-08
   0.0000000e+00  3.3457618e-05]]
2.TKTKTKTK::::::: [[-0.00000000e+00 -1.29248510e-05  1.98058733e-05 ... -1.29248510e-05
   1.98058733e-05 -0.00000000e+00]
 [ 0.00000000e+00 -1.29248510e-05 -0.00000000e+00 ...  0.00000000e+00
   1.98058733e-05  1.98058733e-05]
 [-1.29248510e-05  0.00000000e+00 -0.00000000e+00 ...  0.00000000e+00
  -0.00000000e+00 -0.00000000e+00]
 ...
 [-3.42882813e-05  3.34576180e-05  3.34576180e-05 ...  2.01224797e-08
   2.01224797e-08  2.01224797e-08]
 [ 2.01224797e-08  3.34576180e-05  3.34576180e-05 ...  2.01224797e-08
  -0.00000000e+00  2.01224797e-08]
 [ 3.34576180e-05  7.83659780e-05  2.01224797e-08 ...  2.01224797e-08
   0.00000000e+00  3.34576180e-05]]
3.TKTKTKTK::::::: [[  1   3   4 ...   3   4   1]
 [  5   3   2 ...   5   4   4]
 [  3   5   2 ...   6   2   2]
 ...
 [484 485 485 ... 486 486 486]
 [486 485 485 ... 486 482 486]
 [485 483 486 ... 486 481 485]]
ncluster/block is 8
nhWeight/block is 200
!!!!!!!!!!!nBlock!!!!! see this !!!5
!!!!!!!!sparse ratio!!!!!!!! see this !!!6.0
blocked!!!
!!!!!!!wantto!!!!!!!!!! see this !!!0.25
!!!!!!!!!but!!!!!! see this !!!0.0831
1.TKTKTKTK::::::: [[-0.0389565  -0.0389565   0.26544023 ... -0.0389565  -0.0389565
   0.07410996]
 [ 0.26544023 -0.11477213  0.07410996 ... -0.11477213 -0.0389565
  -0.0389565 ]
 [ 0.07410996 -0.0389565  -0.0389565  ... -0.0389565  -0.0389565
  -0.0389565 ]
 ...
 [-0.16967633 -0.16967633 -0.16967633 ... -0.16967633 -0.16967633
   0.14513288]
 [-0.01289844 -0.01289844 -0.16967633 ... -0.01289844 -0.01289844
  -0.01289844]
 [-0.01289844 -0.01289844 -0.16967633 ... -0.01289844 -0.01289844
  -0.16967633]]
2.TKTKTKTK::::::: [[-0.0389565  -0.0389565   0.         ... -0.0389565  -0.0389565
   0.07410996]
 [ 0.         -0.11477213  0.07410996 ... -0.11477213 -0.0389565
  -0.0389565 ]
 [ 0.07410996 -0.0389565  -0.0389565  ... -0.0389565  -0.0389565
  -0.0389565 ]
 ...
 [-0.16967633 -0.16967633 -0.16967633 ... -0.16967633 -0.16967633
   0.14513288]
 [-0.01289844 -0.01289844 -0.16967633 ... -0.01289844 -0.01289844
  -0.01289844]
 [-0.01289844 -0.01289844 -0.16967633 ... -0.01289844 -0.01289844
  -0.16967633]]
3.TKTKTKTK::::::: [[ 4  4  5 ...  4  4  1]
 [ 5  3  1 ...  3  4  4]
 [ 1  4  4 ...  4  4  4]
 ...
 [34 34 34 ... 34 34 37]
 [38 38 34 ... 38 38 38]
 [38 38 34 ... 38 38 34]]
debug1.................................................add_forward__graph : end
debug2.................................................add_loss_graph : end
tk:grads_vars is  [(<tf.Tensor 'gradients/AddN_1:0' shape=(784, 1000) dtype=float32>, <tf.Variable 'dense1/weights:0' shape=(784, 1000) dtype=float32_ref>), (<tf.Tensor 'gradients/dense1/BiasAdd_grad/tuple/control_dependency_1:0' shape=(1000,) dtype=float32>, <tf.Variable 'dense1/biases:0' shape=(1000,) dtype=float32_ref>), (<tf.Tensor 'gradients/AddN:0' shape=(1000, 10) dtype=float32>, <tf.Variable 'dense2/weights:0' shape=(1000, 10) dtype=float32_ref>), (<tf.Tensor 'gradients/dense2/BiasAdd_grad/tuple/control_dependency_1:0' shape=(10,) dtype=float32>, <tf.Variable 'dense2/biases:0' shape=(10,) dtype=float32_ref>)]
0 trainable variable is !!!grad: Tensor("gradients/AddN_1:0", shape=(784, 1000), dtype=float32, device=/device:GPU:0)
0 trainable variable is !!!var: <tf.Variable 'dense1/weights:0' shape=(784, 1000) dtype=float32_ref>
1 trainable variable is !!!grad: Tensor("gradients/dense1/BiasAdd_grad/tuple/control_dependency_1:0", shape=(1000,), dtype=float32, device=/device:GPU:0)
1 trainable variable is !!!var: <tf.Variable 'dense1/biases:0' shape=(1000,) dtype=float32_ref>
2 trainable variable is !!!grad: Tensor("gradients/AddN:0", shape=(1000, 10), dtype=float32, device=/device:GPU:0)
2 trainable variable is !!!var: <tf.Variable 'dense2/weights:0' shape=(1000, 10) dtype=float32_ref>
3 trainable variable is !!!grad: Tensor("gradients/dense2/BiasAdd_grad/tuple/control_dependency_1:0", shape=(10,), dtype=float32, device=/device:GPU:0)
3 trainable variable is !!!var: <tf.Variable 'dense2/biases:0' shape=(10,) dtype=float32_ref>
debug3...............................................add_train_graph : end
Successfully Imported ../data/MNIST/train-images.gz
Extracting ../data/MNIST/train-images.gz
Successfully Imported ../data/MNIST/train-labels.gz
Extracting ../data/MNIST/train-labels.gz
Successfully Imported ../data/MNIST/test-images.gz
Extracting ../data/MNIST/test-images.gz
Successfully Imported ../data/MNIST/test-labels.gz
Extracting ../data/MNIST/test-labels.gz
Accuracy :  0.2662000025808811
-----------what is loss : 2.9815144538879395
2019-11-12 15:06:46.196897: step 0, loss = 2.98 (1346.7 images/sec; 0.074 sec/batch)
-----------what is loss : 1.3554378747940063
2019-11-12 15:06:46.341292: step 10, loss = 1.36 (3167.3 images/sec; 0.032 sec/batch)
-----------what is loss : 1.3859057426452637
2019-11-12 15:06:46.463340: step 20, loss = 1.39 (3176.4 images/sec; 0.031 sec/batch)
-----------what is loss : 1.2065809965133667
2019-11-12 15:06:46.586906: step 30, loss = 1.21 (3103.6 images/sec; 0.032 sec/batch)
-----------what is loss : 1.1421799659729004
2019-11-12 15:06:46.709860: step 40, loss = 1.14 (3207.0 images/sec; 0.031 sec/batch)
-----------what is loss : 1.0710008144378662
2019-11-12 15:06:46.831722: step 50, loss = 1.07 (3228.3 images/sec; 0.031 sec/batch)
-----------what is loss : 1.1926196813583374
2019-11-12 15:06:46.954204: step 60, loss = 1.19 (3146.0 images/sec; 0.032 sec/batch)
-----------what is loss : 0.8581663370132446
2019-11-12 15:06:47.077484: step 70, loss = 0.86 (3100.9 images/sec; 0.032 sec/batch)
-----------what is loss : 0.9016429781913757
2019-11-12 15:06:47.199339: step 80, loss = 0.90 (3199.2 images/sec; 0.031 sec/batch)
-----------what is loss : 1.0710922479629517
2019-11-12 15:06:47.321647: step 90, loss = 1.07 (3158.1 images/sec; 0.032 sec/batch)
-----------what is loss : 0.8996373414993286
2019-11-12 15:06:47.450285: step 100, loss = 0.90 (2725.6 images/sec; 0.037 sec/batch)
-----------what is loss : 0.951434314250946
2019-11-12 15:06:47.574373: step 110, loss = 0.95 (3133.7 images/sec; 0.032 sec/batch)
-----------what is loss : 0.9525018334388733
2019-11-12 15:06:47.697437: step 120, loss = 0.95 (3157.4 images/sec; 0.032 sec/batch)
-----------what is loss : 1.1424479484558105
2019-11-12 15:06:47.820242: step 130, loss = 1.14 (3136.0 images/sec; 0.032 sec/batch)
-----------what is loss : 0.9744009971618652
2019-11-12 15:06:47.942887: step 140, loss = 0.97 (3140.4 images/sec; 0.032 sec/batch)
-----------what is loss : 0.8168889284133911
2019-11-12 15:06:48.065864: step 150, loss = 0.82 (3161.6 images/sec; 0.032 sec/batch)
-----------what is loss : 0.8665710687637329
2019-11-12 15:06:48.187302: step 160, loss = 0.87 (3194.6 images/sec; 0.031 sec/batch)
-----------what is loss : 0.9280048608779907
2019-11-12 15:06:48.309665: step 170, loss = 0.93 (3170.6 images/sec; 0.032 sec/batch)
-----------what is loss : 0.8868714570999146
2019-11-12 15:06:48.432615: step 180, loss = 0.89 (3144.9 images/sec; 0.032 sec/batch)
-----------what is loss : 0.8938972353935242
2019-11-12 15:06:48.555355: step 190, loss = 0.89 (3109.4 images/sec; 0.032 sec/batch)
-----------what is loss : 0.7758020162582397
2019-11-12 15:06:48.677578: step 200, loss = 0.78 (3157.9 images/sec; 0.032 sec/batch)
-----------what is loss : 1.077942967414856
2019-11-12 15:06:48.799884: step 210, loss = 1.08 (3232.1 images/sec; 0.031 sec/batch)
-----------what is loss : 1.0442079305648804
2019-11-12 15:06:48.922363: step 220, loss = 1.04 (3174.5 images/sec; 0.032 sec/batch)
-----------what is loss : 0.7855673432350159
2019-11-12 15:06:49.044632: step 230, loss = 0.79 (3200.9 images/sec; 0.031 sec/batch)
-----------what is loss : 0.8863860964775085
2019-11-12 15:06:49.166177: step 240, loss = 0.89 (3198.7 images/sec; 0.031 sec/batch)
-----------what is loss : 0.8600303530693054
2019-11-12 15:06:49.287663: step 250, loss = 0.86 (3181.9 images/sec; 0.031 sec/batch)
-----------what is loss : 0.7956211566925049
2019-11-12 15:06:49.471567: step 260, loss = 0.80 (1080.8 images/sec; 0.093 sec/batch)
-----------what is loss : 4.591677188873291
2019-11-12 15:06:49.599142: step 270, loss = 4.59 (3197.8 images/sec; 0.031 sec/batch)
-----------what is loss : 3.9941649436950684
2019-11-12 15:06:49.721054: step 280, loss = 3.99 (3178.9 images/sec; 0.031 sec/batch)
-----------what is loss : 3.200876235961914
2019-11-12 15:06:49.843069: step 290, loss = 3.20 (3256.0 images/sec; 0.031 sec/batch)
-----------what is loss : 2.92120623588562
2019-11-12 15:06:49.964451: step 300, loss = 2.92 (3276.5 images/sec; 0.031 sec/batch)
-----------what is loss : 2.8353617191314697
2019-11-12 15:06:50.085320: step 310, loss = 2.84 (3258.6 images/sec; 0.031 sec/batch)
-----------what is loss : 2.732856273651123
2019-11-12 15:06:50.207264: step 320, loss = 2.73 (3350.5 images/sec; 0.030 sec/batch)
-----------what is loss : 2.618502378463745
2019-11-12 15:06:50.327837: step 330, loss = 2.62 (3310.8 images/sec; 0.030 sec/batch)
-----------what is loss : 2.6387181282043457
2019-11-12 15:06:50.448376: step 340, loss = 2.64 (3327.4 images/sec; 0.030 sec/batch)
-----------what is loss : 2.5477373600006104
2019-11-12 15:06:50.574260: step 350, loss = 2.55 (2819.1 images/sec; 0.035 sec/batch)
-----------what is loss : 2.5830914974212646
2019-11-12 15:06:50.694739: step 360, loss = 2.58 (3377.7 images/sec; 0.030 sec/batch)
-----------what is loss : 2.464308977127075
2019-11-12 15:06:50.814816: step 370, loss = 2.46 (3350.0 images/sec; 0.030 sec/batch)
-----------what is loss : 2.4570565223693848
2019-11-12 15:06:50.935595: step 380, loss = 2.46 (3318.8 images/sec; 0.030 sec/batch)
-----------what is loss : 2.4980006217956543
2019-11-12 15:06:51.056308: step 390, loss = 2.50 (3372.9 images/sec; 0.030 sec/batch)
-----------what is loss : 2.4882211685180664
2019-11-12 15:06:51.177628: step 400, loss = 2.49 (3264.8 images/sec; 0.031 sec/batch)
-----------what is loss : 2.5115950107574463
2019-11-12 15:06:51.298365: step 410, loss = 2.51 (3332.9 images/sec; 0.030 sec/batch)
-----------what is loss : 2.4127120971679688
2019-11-12 15:06:51.420107: step 420, loss = 2.41 (3279.5 images/sec; 0.030 sec/batch)
-----------what is loss : 2.46565318107605
2019-11-12 15:06:51.542165: step 430, loss = 2.47 (3284.6 images/sec; 0.030 sec/batch)
-----------what is loss : 2.4501523971557617
2019-11-12 15:06:51.668264: step 440, loss = 2.45 (3254.2 images/sec; 0.031 sec/batch)
-----------what is loss : 2.4095678329467773
2019-11-12 15:06:51.792445: step 450, loss = 2.41 (3278.7 images/sec; 0.030 sec/batch)
-----------what is loss : 2.5020792484283447
2019-11-12 15:06:51.922060: step 460, loss = 2.50 (2850.2 images/sec; 0.035 sec/batch)
-----------what is loss : 2.398895502090454
2019-11-12 15:06:52.047621: step 470, loss = 2.40 (3296.0 images/sec; 0.030 sec/batch)
-----------what is loss : 2.4038774967193604
2019-11-12 15:06:52.173225: step 480, loss = 2.40 (3255.7 images/sec; 0.031 sec/batch)
-----------what is loss : 2.3572981357574463
2019-11-12 15:06:52.298972: step 490, loss = 2.36 (3259.2 images/sec; 0.031 sec/batch)
-----------what is loss : 2.403578042984009
2019-11-12 15:06:52.424020: step 500, loss = 2.40 (3307.7 images/sec; 0.030 sec/batch)
-----------what is loss : 2.3981008529663086
2019-11-12 15:06:52.548376: step 510, loss = 2.40 (3300.6 images/sec; 0.030 sec/batch)
-----------what is loss : 2.422743797302246
2019-11-12 15:06:52.672031: step 520, loss = 2.42 (3326.7 images/sec; 0.030 sec/batch)
-----------what is loss : 2.3956055641174316
2019-11-12 15:06:52.853596: step 530, loss = 2.40 (3274.6 images/sec; 0.031 sec/batch)
-----------what is loss : 2.3559064865112305
2019-11-12 15:06:52.977177: step 540, loss = 2.36 (3317.0 images/sec; 0.030 sec/batch)
-----------what is loss : 2.3993091583251953
2019-11-12 15:06:53.107753: step 550, loss = 2.40 (3077.0 images/sec; 0.032 sec/batch)
-----------what is loss : 2.411947250366211
2019-11-12 15:06:53.237781: step 560, loss = 2.41 (2809.2 images/sec; 0.036 sec/batch)
-----------what is loss : 2.407837390899658
2019-11-12 15:06:53.364031: step 570, loss = 2.41 (3348.0 images/sec; 0.030 sec/batch)
-----------what is loss : 2.336759328842163
2019-11-12 15:06:53.493183: step 580, loss = 2.34 (2847.4 images/sec; 0.035 sec/batch)
-----------what is loss : 2.403358221054077
2019-11-12 15:06:53.617966: step 590, loss = 2.40 (3296.0 images/sec; 0.030 sec/batch)
-----------what is loss : 2.3792471885681152
2019-11-12 15:06:53.739670: step 600, loss = 2.38 (3360.9 images/sec; 0.030 sec/batch)
-----------what is loss : 2.382455348968506
2019-11-12 15:06:53.868054: step 610, loss = 2.38 (3306.7 images/sec; 0.030 sec/batch)
-----------what is loss : 2.387664794921875
2019-11-12 15:06:53.991660: step 620, loss = 2.39 (3335.6 images/sec; 0.030 sec/batch)
-----------what is loss : 2.411437749862671
2019-11-12 15:06:54.113515: step 630, loss = 2.41 (3331.9 images/sec; 0.030 sec/batch)
-----------what is loss : 2.3868350982666016
2019-11-12 15:06:54.235232: step 640, loss = 2.39 (3388.3 images/sec; 0.030 sec/batch)
-----------what is loss : 2.378723382949829
2019-11-12 15:06:54.357442: step 650, loss = 2.38 (3366.4 images/sec; 0.030 sec/batch)
-----------what is loss : 2.38733172416687
2019-11-12 15:06:54.479459: step 660, loss = 2.39 (3330.8 images/sec; 0.030 sec/batch)
-----------what is loss : 2.3748974800109863
2019-11-12 15:06:54.601835: step 670, loss = 2.37 (3309.3 images/sec; 0.030 sec/batch)
-----------what is loss : 2.3667707443237305
2019-11-12 15:06:54.723433: step 680, loss = 2.37 (3353.4 images/sec; 0.030 sec/batch)
-----------what is loss : 2.355433702468872
2019-11-12 15:06:54.844074: step 690, loss = 2.36 (3358.0 images/sec; 0.030 sec/batch)
-----------what is loss : 2.3409173488616943
2019-11-12 15:06:54.966241: step 700, loss = 2.34 (3258.3 images/sec; 0.031 sec/batch)
-----------what is loss : 2.3599894046783447
2019-11-12 15:06:55.089643: step 710, loss = 2.36 (3235.6 images/sec; 0.031 sec/batch)
-----------what is loss : 2.3741021156311035
2019-11-12 15:06:55.211225: step 720, loss = 2.37 (3300.0 images/sec; 0.030 sec/batch)
-----------what is loss : 2.4103848934173584
2019-11-12 15:06:55.333450: step 730, loss = 2.41 (3300.3 images/sec; 0.030 sec/batch)
-----------what is loss : 2.3671040534973145
2019-11-12 15:06:55.454703: step 740, loss = 2.37 (3269.3 images/sec; 0.031 sec/batch)
-----------what is loss : 2.348414897918701
2019-11-12 15:06:55.577750: step 750, loss = 2.35 (3317.2 images/sec; 0.030 sec/batch)
-----------what is loss : 2.353034019470215
2019-11-12 15:06:55.699147: step 760, loss = 2.35 (3324.1 images/sec; 0.030 sec/batch)
-----------what is loss : 2.367389440536499
2019-11-12 15:06:55.819472: step 770, loss = 2.37 (3365.2 images/sec; 0.030 sec/batch)
-----------what is loss : 2.353553533554077
2019-11-12 15:06:55.944647: step 780, loss = 2.35 (3411.8 images/sec; 0.029 sec/batch)
-----------what is loss : 2.3823962211608887
2019-11-12 15:06:56.121674: step 790, loss = 2.38 (3398.2 images/sec; 0.029 sec/batch)
-----------what is loss : 2.320199728012085
2019-11-12 15:06:56.242342: step 800, loss = 2.32 (3339.5 images/sec; 0.030 sec/batch)
-----------what is loss : 2.3339321613311768
2019-11-12 15:06:56.362530: step 810, loss = 2.33 (3377.6 images/sec; 0.030 sec/batch)
-----------what is loss : 2.344820499420166
2019-11-12 15:06:56.484487: step 820, loss = 2.34 (3343.4 images/sec; 0.030 sec/batch)
-----------what is loss : 2.3435940742492676
2019-11-12 15:06:56.605605: step 830, loss = 2.34 (3368.9 images/sec; 0.030 sec/batch)
-----------what is loss : 2.360050916671753
2019-11-12 15:06:56.726179: step 840, loss = 2.36 (3383.5 images/sec; 0.030 sec/batch)
-----------what is loss : 2.334172248840332
2019-11-12 15:06:56.846899: step 850, loss = 2.33 (3334.5 images/sec; 0.030 sec/batch)
-----------what is loss : 2.309561252593994
2019-11-12 15:06:56.966664: step 860, loss = 2.31 (3377.8 images/sec; 0.030 sec/batch)
-----------what is loss : 2.354158639907837
2019-11-12 15:06:57.087498: step 870, loss = 2.35 (3379.7 images/sec; 0.030 sec/batch)
-----------what is loss : 2.3469290733337402
2019-11-12 15:06:57.207976: step 880, loss = 2.35 (3331.2 images/sec; 0.030 sec/batch)
-----------what is loss : 2.3389222621917725
2019-11-12 15:06:57.336606: step 890, loss = 2.34 (2803.3 images/sec; 0.036 sec/batch)
-----------what is loss : 2.363919258117676
2019-11-12 15:06:57.460503: step 900, loss = 2.36 (3276.0 images/sec; 0.031 sec/batch)
-----------what is loss : 2.3579163551330566
2019-11-12 15:06:57.584927: step 910, loss = 2.36 (3289.6 images/sec; 0.030 sec/batch)
-----------what is loss : 2.350999593734741
2019-11-12 15:06:57.709553: step 920, loss = 2.35 (3302.8 images/sec; 0.030 sec/batch)
-----------what is loss : 2.3580498695373535
2019-11-12 15:06:57.834590: step 930, loss = 2.36 (3314.5 images/sec; 0.030 sec/batch)
-----------what is loss : 2.358436107635498
2019-11-12 15:06:57.960624: step 940, loss = 2.36 (3151.1 images/sec; 0.032 sec/batch)
-----------what is loss : 2.354583978652954
2019-11-12 15:06:58.088401: step 950, loss = 2.35 (3321.4 images/sec; 0.030 sec/batch)
-----------what is loss : 2.3880321979522705
2019-11-12 15:06:58.217832: step 960, loss = 2.39 (2871.0 images/sec; 0.035 sec/batch)
-----------what is loss : 2.354443073272705
2019-11-12 15:06:58.342341: step 970, loss = 2.35 (3333.8 images/sec; 0.030 sec/batch)
-----------what is loss : 2.369115114212036
2019-11-12 15:06:58.467098: step 980, loss = 2.37 (3283.0 images/sec; 0.030 sec/batch)
-----------what is loss : 2.337611675262451
2019-11-12 15:06:58.590789: step 990, loss = 2.34 (3343.6 images/sec; 0.030 sec/batch)
optimization Finished
Accuracy :  0.3506000031530857
tuple_tem key is dense2
tuple_tem key is dense1
tuple_tem value is (1000, 10)
tuple_tem value is (10,)
tuple_tem value is (784, 1000)
tuple_tem value is (1000,)
tk is  [[ 1.4746308e-06 -6.7412066e-06  1.0428151e-05 ... -6.7412066e-06
   1.0428151e-05  1.4746308e-06]
 [ 0.0000000e+00 -6.7412066e-06 -0.0000000e+00 ...  0.0000000e+00
   1.0428151e-05  1.0428151e-05]
 [-6.7412066e-06  0.0000000e+00 -0.0000000e+00 ...  3.2692994e-06
  -0.0000000e+00 -0.0000000e+00]
 ...
 [-1.8855872e-05  1.8399098e-05  1.8399098e-05 ...  1.1065798e-08
   1.1065798e-08  1.1065798e-08]
 [ 1.1065798e-08  1.8399098e-05  1.8399098e-05 ...  1.1065798e-08
  -0.0000000e+00  1.1065798e-08]
 [ 1.8399098e-05  4.3095180e-05  1.1065798e-08 ...  1.1065798e-08
   0.0000000e+00  1.8399098e-05]]
tk is  [[  1   3   4 ...   3   4   1]
 [  5   3   2 ...   5   4   4]
 [  3   5   2 ...   6   2   2]
 ...
 [484 485 485 ... 486 486 486]
 [486 485 485 ... 486 482 486]
 [485 483 486 ... 486 481 485]]
tk is  [[-0.01806451 -0.01806451  0.         ... -0.01806451 -0.01806451
   0.03267927]
 [ 0.         -0.06258675  0.03267927 ... -0.06258675 -0.01806451
  -0.01806451]
 [ 0.03267927 -0.01806451 -0.01806451 ... -0.01806451 -0.01806451
  -0.01806451]
 ...
 [-0.0974174  -0.0974174  -0.0974174  ... -0.0974174  -0.0974174
   0.07629506]
 [-0.00704044 -0.00704044 -0.0974174  ... -0.00704044 -0.00704044
  -0.00704044]
 [-0.00704044 -0.00704044 -0.0974174  ... -0.00704044 -0.00704044
  -0.0974174 ]]
tk is  [[ 4  4  5 ...  4  4  1]
 [ 5  3  1 ...  3  4  4]
 [ 1  4  4 ...  4  4  4]
 ...
 [34 34 34 ... 34 34 37]
 [38 38 34 ... 38 38 38]
 [38 38 34 ... 38 38 34]]
tk2 is  True
tk2 is  5
=============================================0.75===========================================
mc.LOAD_PRETRAINED_MODEL is True
OK to save ./MNIST/train/MNIST_PRUNING_INFO.pkl
ncluster/block is 8
nhWeight/block is 13
!!!!!!!!!!!nBlock!!!!! see this !!!61
!!!!!!!!sparse ratio!!!!!!!! see this !!!6.0
blocked!!!
!!!!!!!wantto!!!!!!!!!! see this !!!0.25
!!!!!!!!!but!!!!!! see this !!!0.23475127551020408
1.TKTKTKTK::::::: [[ 1.4746308e-06 -6.7412066e-06  1.0428151e-05 ... -6.7412066e-06
   1.0428151e-05  1.4746308e-06]
 [ 0.0000000e+00 -6.7412066e-06 -0.0000000e+00 ...  0.0000000e+00
   1.0428151e-05  1.0428151e-05]
 [-6.7412066e-06  0.0000000e+00 -0.0000000e+00 ...  3.2692994e-06
  -0.0000000e+00 -0.0000000e+00]
 ...
 [-1.8855872e-05  1.8399098e-05  1.8399098e-05 ...  1.1065798e-08
   1.1065798e-08  1.1065798e-08]
 [ 1.1065798e-08  1.8399098e-05  1.8399098e-05 ...  1.1065798e-08
  -0.0000000e+00  1.1065798e-08]
 [ 1.8399098e-05  4.3095180e-05  1.1065798e-08 ...  1.1065798e-08
   0.0000000e+00  1.8399098e-05]]
2.TKTKTKTK::::::: [[ 1.47463084e-06 -6.74120656e-06  0.00000000e+00 ... -6.74120656e-06
   0.00000000e+00  1.47463084e-06]
 [ 0.00000000e+00 -6.74120656e-06 -0.00000000e+00 ...  0.00000000e+00
   0.00000000e+00  0.00000000e+00]
 [-6.74120656e-06  0.00000000e+00 -0.00000000e+00 ...  3.26929944e-06
  -0.00000000e+00 -0.00000000e+00]
 ...
 [-1.88558715e-05  1.83990978e-05  1.83990978e-05 ...  1.10657981e-08
   1.10657981e-08  1.10657981e-08]
 [ 1.10657981e-08  1.83990978e-05  1.83990978e-05 ...  1.10657981e-08
  -0.00000000e+00  1.10657981e-08]
 [ 1.83990978e-05  0.00000000e+00  1.10657981e-08 ...  1.10657981e-08
   0.00000000e+00  1.83990978e-05]]
3.TKTKTKTK::::::: [[  1   3   4 ...   3   4   1]
 [  5   3   2 ...   5   4   4]
 [  3   5   2 ...   6   2   2]
 ...
 [484 485 485 ... 486 486 486]
 [486 485 485 ... 486 482 486]
 [485 483 486 ... 486 481 485]]
ncluster/block is 8
nhWeight/block is 200
!!!!!!!!!!!nBlock!!!!! see this !!!5
!!!!!!!!sparse ratio!!!!!!!! see this !!!6.0
blocked!!!
!!!!!!!wantto!!!!!!!!!! see this !!!0.25
!!!!!!!!!but!!!!!! see this !!!0.1905
1.TKTKTKTK::::::: [[-0.01806451 -0.01806451  0.         ... -0.01806451 -0.01806451
   0.03267927]
 [ 0.         -0.06258675  0.03267927 ... -0.06258675 -0.01806451
  -0.01806451]
 [ 0.03267927 -0.01806451 -0.01806451 ... -0.01806451 -0.01806451
  -0.01806451]
 ...
 [-0.0974174  -0.0974174  -0.0974174  ... -0.0974174  -0.0974174
   0.07629506]
 [-0.00704044 -0.00704044 -0.0974174  ... -0.00704044 -0.00704044
  -0.00704044]
 [-0.00704044 -0.00704044 -0.0974174  ... -0.00704044 -0.00704044
  -0.0974174 ]]
2.TKTKTKTK::::::: [[-0.01806451 -0.01806451  0.         ... -0.01806451 -0.01806451
   0.03267927]
 [ 0.         -0.          0.03267927 ... -0.         -0.01806451
  -0.01806451]
 [ 0.03267927 -0.01806451 -0.01806451 ... -0.01806451 -0.01806451
  -0.01806451]
 ...
 [-0.         -0.         -0.         ... -0.         -0.
   0.07629506]
 [-0.00704044 -0.00704044 -0.         ... -0.00704044 -0.00704044
  -0.00704044]
 [-0.00704044 -0.00704044 -0.         ... -0.00704044 -0.00704044
  -0.        ]]
3.TKTKTKTK::::::: [[ 4  4  5 ...  4  4  1]
 [ 5  3  1 ...  3  4  4]
 [ 1  4  4 ...  4  4  4]
 ...
 [34 34 34 ... 34 34 37]
 [38 38 34 ... 38 38 38]
 [38 38 34 ... 38 38 34]]
debug1.................................................add_forward__graph : end
debug2.................................................add_loss_graph : end
tk:grads_vars is  [(<tf.Tensor 'gradients/AddN_1:0' shape=(784, 1000) dtype=float32>, <tf.Variable 'dense1/weights:0' shape=(784, 1000) dtype=float32_ref>), (<tf.Tensor 'gradients/dense1/BiasAdd_grad/tuple/control_dependency_1:0' shape=(1000,) dtype=float32>, <tf.Variable 'dense1/biases:0' shape=(1000,) dtype=float32_ref>), (<tf.Tensor 'gradients/AddN:0' shape=(1000, 10) dtype=float32>, <tf.Variable 'dense2/weights:0' shape=(1000, 10) dtype=float32_ref>), (<tf.Tensor 'gradients/dense2/BiasAdd_grad/tuple/control_dependency_1:0' shape=(10,) dtype=float32>, <tf.Variable 'dense2/biases:0' shape=(10,) dtype=float32_ref>)]
0 trainable variable is !!!grad: Tensor("gradients/AddN_1:0", shape=(784, 1000), dtype=float32, device=/device:GPU:0)
0 trainable variable is !!!var: <tf.Variable 'dense1/weights:0' shape=(784, 1000) dtype=float32_ref>
1 trainable variable is !!!grad: Tensor("gradients/dense1/BiasAdd_grad/tuple/control_dependency_1:0", shape=(1000,), dtype=float32, device=/device:GPU:0)
1 trainable variable is !!!var: <tf.Variable 'dense1/biases:0' shape=(1000,) dtype=float32_ref>
2 trainable variable is !!!grad: Tensor("gradients/AddN:0", shape=(1000, 10), dtype=float32, device=/device:GPU:0)
2 trainable variable is !!!var: <tf.Variable 'dense2/weights:0' shape=(1000, 10) dtype=float32_ref>
3 trainable variable is !!!grad: Tensor("gradients/dense2/BiasAdd_grad/tuple/control_dependency_1:0", shape=(10,), dtype=float32, device=/device:GPU:0)
3 trainable variable is !!!var: <tf.Variable 'dense2/biases:0' shape=(10,) dtype=float32_ref>
debug3...............................................add_train_graph : end
Successfully Imported ../data/MNIST/train-images.gz
Extracting ../data/MNIST/train-images.gz
Successfully Imported ../data/MNIST/train-labels.gz
Extracting ../data/MNIST/train-labels.gz
Successfully Imported ../data/MNIST/test-images.gz
Extracting ../data/MNIST/test-images.gz
Successfully Imported ../data/MNIST/test-labels.gz
Extracting ../data/MNIST/test-labels.gz
Accuracy :  0.09799999993294478
-----------what is loss : 3.746649742126465
2019-11-12 15:07:03.171219: step 0, loss = 3.75 (1404.8 images/sec; 0.071 sec/batch)
-----------what is loss : 2.6177806854248047
2019-11-12 15:07:03.312111: step 10, loss = 2.62 (3480.2 images/sec; 0.029 sec/batch)
-----------what is loss : 2.3219735622406006
2019-11-12 15:07:03.436083: step 20, loss = 2.32 (2913.7 images/sec; 0.034 sec/batch)
-----------what is loss : 2.2732889652252197
2019-11-12 15:07:03.554307: step 30, loss = 2.27 (3449.3 images/sec; 0.029 sec/batch)
-----------what is loss : 2.2311935424804688
2019-11-12 15:07:03.672267: step 40, loss = 2.23 (3526.4 images/sec; 0.028 sec/batch)
-----------what is loss : 2.161550760269165
2019-11-12 15:07:03.789099: step 50, loss = 2.16 (3533.9 images/sec; 0.028 sec/batch)
-----------what is loss : 2.1583092212677
2019-11-12 15:07:03.906757: step 60, loss = 2.16 (3503.0 images/sec; 0.029 sec/batch)
-----------what is loss : 2.0728278160095215
2019-11-12 15:07:04.023584: step 70, loss = 2.07 (3501.3 images/sec; 0.029 sec/batch)
-----------what is loss : 2.044834613800049
2019-11-12 15:07:04.140733: step 80, loss = 2.04 (3484.8 images/sec; 0.029 sec/batch)
-----------what is loss : 2.045445680618286
2019-11-12 15:07:04.258052: step 90, loss = 2.05 (3523.6 images/sec; 0.028 sec/batch)
-----------what is loss : 1.963329553604126
2019-11-12 15:07:04.376492: step 100, loss = 1.96 (3422.2 images/sec; 0.029 sec/batch)
-----------what is loss : 1.8930840492248535
2019-11-12 15:07:04.496126: step 110, loss = 1.89 (3370.4 images/sec; 0.030 sec/batch)
-----------what is loss : 1.8705781698226929
2019-11-12 15:07:04.613638: step 120, loss = 1.87 (3433.4 images/sec; 0.029 sec/batch)
-----------what is loss : 1.8277300596237183
2019-11-12 15:07:04.732176: step 130, loss = 1.83 (3439.0 images/sec; 0.029 sec/batch)
-----------what is loss : 1.7645161151885986
2019-11-12 15:07:04.850741: step 140, loss = 1.76 (3419.0 images/sec; 0.029 sec/batch)
-----------what is loss : 1.6303963661193848
2019-11-12 15:07:04.968269: step 150, loss = 1.63 (3426.4 images/sec; 0.029 sec/batch)
-----------what is loss : 1.7164634466171265
2019-11-12 15:07:05.086385: step 160, loss = 1.72 (3358.6 images/sec; 0.030 sec/batch)
-----------what is loss : 1.5046831369400024
2019-11-12 15:07:05.204188: step 170, loss = 1.50 (3383.7 images/sec; 0.030 sec/batch)
-----------what is loss : 1.5203659534454346
2019-11-12 15:07:05.322674: step 180, loss = 1.52 (3392.0 images/sec; 0.029 sec/batch)
-----------what is loss : 1.43305504322052
2019-11-12 15:07:05.441206: step 190, loss = 1.43 (3344.7 images/sec; 0.030 sec/batch)
-----------what is loss : 1.2987048625946045
2019-11-12 15:07:05.559380: step 200, loss = 1.30 (3314.7 images/sec; 0.030 sec/batch)
-----------what is loss : 1.2796353101730347
2019-11-12 15:07:05.677584: step 210, loss = 1.28 (3392.8 images/sec; 0.029 sec/batch)
-----------what is loss : 1.2815799713134766
2019-11-12 15:07:05.796512: step 220, loss = 1.28 (3319.3 images/sec; 0.030 sec/batch)
-----------what is loss : 1.0976636409759521
2019-11-12 15:07:05.914284: step 230, loss = 1.10 (3326.4 images/sec; 0.030 sec/batch)
-----------what is loss : 1.1589752435684204
2019-11-12 15:07:06.032807: step 240, loss = 1.16 (3383.8 images/sec; 0.030 sec/batch)
-----------what is loss : 1.187544345855713
2019-11-12 15:07:06.152418: step 250, loss = 1.19 (3230.5 images/sec; 0.031 sec/batch)
-----------what is loss : 0.9612579941749573
2019-11-12 15:07:06.337082: step 260, loss = 0.96 (1042.5 images/sec; 0.096 sec/batch)
-----------what is loss : 1.0817674398422241
2019-11-12 15:07:06.457833: step 270, loss = 1.08 (3339.0 images/sec; 0.030 sec/batch)
-----------what is loss : 1.063997507095337
2019-11-12 15:07:06.578342: step 280, loss = 1.06 (3192.2 images/sec; 0.031 sec/batch)
-----------what is loss : 1.1060054302215576
2019-11-12 15:07:06.697017: step 290, loss = 1.11 (3202.6 images/sec; 0.031 sec/batch)
-----------what is loss : 0.8366169333457947
2019-11-12 15:07:06.816298: step 300, loss = 0.84 (3328.5 images/sec; 0.030 sec/batch)
-----------what is loss : 0.9455148577690125
2019-11-12 15:07:06.937106: step 310, loss = 0.95 (3124.5 images/sec; 0.032 sec/batch)
-----------what is loss : 1.0399845838546753
2019-11-12 15:07:07.056858: step 320, loss = 1.04 (3250.1 images/sec; 0.031 sec/batch)
-----------what is loss : 0.9102537035942078
2019-11-12 15:07:07.175728: step 330, loss = 0.91 (3276.3 images/sec; 0.031 sec/batch)
-----------what is loss : 0.9570356607437134
2019-11-12 15:07:07.295813: step 340, loss = 0.96 (3238.9 images/sec; 0.031 sec/batch)
-----------what is loss : 0.9723237752914429
2019-11-12 15:07:07.417380: step 350, loss = 0.97 (3207.5 images/sec; 0.031 sec/batch)
-----------what is loss : 1.0003801584243774
2019-11-12 15:07:07.536868: step 360, loss = 1.00 (3272.7 images/sec; 0.031 sec/batch)
-----------what is loss : 0.9567967653274536
2019-11-12 15:07:07.655746: step 370, loss = 0.96 (3312.0 images/sec; 0.030 sec/batch)
-----------what is loss : 0.9060220122337341
2019-11-12 15:07:07.775969: step 380, loss = 0.91 (3196.2 images/sec; 0.031 sec/batch)
-----------what is loss : 0.8056919574737549
2019-11-12 15:07:07.897761: step 390, loss = 0.81 (3150.6 images/sec; 0.032 sec/batch)
-----------what is loss : 0.8017045259475708
2019-11-12 15:07:08.022487: step 400, loss = 0.80 (2816.6 images/sec; 0.036 sec/batch)
-----------what is loss : 0.6813926100730896
2019-11-12 15:07:08.143275: step 410, loss = 0.68 (3176.3 images/sec; 0.031 sec/batch)
-----------what is loss : 0.7246482968330383
2019-11-12 15:07:08.264158: step 420, loss = 0.72 (3186.8 images/sec; 0.031 sec/batch)
-----------what is loss : 0.746715247631073
2019-11-12 15:07:08.384983: step 430, loss = 0.75 (3285.6 images/sec; 0.030 sec/batch)
-----------what is loss : 0.7772492170333862
2019-11-12 15:07:08.506066: step 440, loss = 0.78 (3261.5 images/sec; 0.031 sec/batch)
-----------what is loss : 0.7281366586685181
2019-11-12 15:07:08.625555: step 450, loss = 0.73 (3225.3 images/sec; 0.031 sec/batch)
-----------what is loss : 0.655537486076355
2019-11-12 15:07:08.746970: step 460, loss = 0.66 (3217.0 images/sec; 0.031 sec/batch)
-----------what is loss : 0.7141661643981934
2019-11-12 15:07:08.868194: step 470, loss = 0.71 (3315.6 images/sec; 0.030 sec/batch)
-----------what is loss : 0.8210335969924927
2019-11-12 15:07:08.990597: step 480, loss = 0.82 (3218.3 images/sec; 0.031 sec/batch)
-----------what is loss : 0.7843585014343262
2019-11-12 15:07:09.111876: step 490, loss = 0.78 (3227.5 images/sec; 0.031 sec/batch)
-----------what is loss : 0.7037633061408997
2019-11-12 15:07:09.232363: step 500, loss = 0.70 (3216.5 images/sec; 0.031 sec/batch)
-----------what is loss : 0.7597231864929199
2019-11-12 15:07:09.352890: step 510, loss = 0.76 (3208.1 images/sec; 0.031 sec/batch)
-----------what is loss : 0.8011096715927124
2019-11-12 15:07:09.470985: step 520, loss = 0.80 (3278.5 images/sec; 0.031 sec/batch)
-----------what is loss : 0.6976276636123657
2019-11-12 15:07:09.646715: step 530, loss = 0.70 (3215.2 images/sec; 0.031 sec/batch)
-----------what is loss : 0.970826268196106
2019-11-12 15:07:09.768315: step 540, loss = 0.97 (3235.9 images/sec; 0.031 sec/batch)
-----------what is loss : 0.8428786993026733
2019-11-12 15:07:09.888881: step 550, loss = 0.84 (3190.2 images/sec; 0.031 sec/batch)
-----------what is loss : 0.7300569415092468
2019-11-12 15:07:10.009893: step 560, loss = 0.73 (3191.5 images/sec; 0.031 sec/batch)
-----------what is loss : 0.6054680347442627
2019-11-12 15:07:10.136425: step 570, loss = 0.61 (2743.1 images/sec; 0.036 sec/batch)
-----------what is loss : 0.6434240341186523
2019-11-12 15:07:10.258012: step 580, loss = 0.64 (3161.2 images/sec; 0.032 sec/batch)
-----------what is loss : 0.7905738949775696
2019-11-12 15:07:10.377890: step 590, loss = 0.79 (3214.8 images/sec; 0.031 sec/batch)
-----------what is loss : 0.9558802843093872
2019-11-12 15:07:10.504426: step 600, loss = 0.96 (2736.7 images/sec; 0.037 sec/batch)
-----------what is loss : 0.6737852096557617
2019-11-12 15:07:10.623729: step 610, loss = 0.67 (3233.9 images/sec; 0.031 sec/batch)
-----------what is loss : 0.6285810470581055
2019-11-12 15:07:10.743698: step 620, loss = 0.63 (3245.5 images/sec; 0.031 sec/batch)
-----------what is loss : 0.6588125228881836
2019-11-12 15:07:10.863608: step 630, loss = 0.66 (3214.3 images/sec; 0.031 sec/batch)
-----------what is loss : 0.6815459132194519
2019-11-12 15:07:10.985348: step 640, loss = 0.68 (3206.7 images/sec; 0.031 sec/batch)
-----------what is loss : 0.7793542146682739
2019-11-12 15:07:11.104989: step 650, loss = 0.78 (3245.6 images/sec; 0.031 sec/batch)
-----------what is loss : 0.8716025352478027
2019-11-12 15:07:11.223917: step 660, loss = 0.87 (3248.4 images/sec; 0.031 sec/batch)
-----------what is loss : 0.6676715016365051
2019-11-12 15:07:11.349721: step 670, loss = 0.67 (2707.1 images/sec; 0.037 sec/batch)
-----------what is loss : 0.7371043562889099
2019-11-12 15:07:11.470447: step 680, loss = 0.74 (3182.9 images/sec; 0.031 sec/batch)
-----------what is loss : 0.6656355857849121
2019-11-12 15:07:11.589511: step 690, loss = 0.67 (3194.9 images/sec; 0.031 sec/batch)
-----------what is loss : 0.8538454174995422
2019-11-12 15:07:11.714862: step 700, loss = 0.85 (2783.8 images/sec; 0.036 sec/batch)
-----------what is loss : 0.7963666915893555
2019-11-12 15:07:11.834822: step 710, loss = 0.80 (3207.6 images/sec; 0.031 sec/batch)
-----------what is loss : 0.6771077513694763
2019-11-12 15:07:11.954721: step 720, loss = 0.68 (3259.8 images/sec; 0.031 sec/batch)
-----------what is loss : 0.6416900157928467
2019-11-12 15:07:12.074550: step 730, loss = 0.64 (3212.5 images/sec; 0.031 sec/batch)
-----------what is loss : 0.7611531019210815
2019-11-12 15:07:12.194057: step 740, loss = 0.76 (3191.7 images/sec; 0.031 sec/batch)
-----------what is loss : 0.7178934216499329
2019-11-12 15:07:12.313376: step 750, loss = 0.72 (3252.2 images/sec; 0.031 sec/batch)
-----------what is loss : 0.8137528300285339
2019-11-12 15:07:12.432367: step 760, loss = 0.81 (3288.6 images/sec; 0.030 sec/batch)
-----------what is loss : 0.5844876766204834
2019-11-12 15:07:12.552547: step 770, loss = 0.58 (3263.0 images/sec; 0.031 sec/batch)
-----------what is loss : 0.7902857661247253
2019-11-12 15:07:12.672372: step 780, loss = 0.79 (3216.6 images/sec; 0.031 sec/batch)
-----------what is loss : 0.5741990208625793
2019-11-12 15:07:12.846864: step 790, loss = 0.57 (3220.1 images/sec; 0.031 sec/batch)
-----------what is loss : 0.6916783452033997
2019-11-12 15:07:12.966627: step 800, loss = 0.69 (3224.7 images/sec; 0.031 sec/batch)
-----------what is loss : 0.6145066022872925
2019-11-12 15:07:13.087138: step 810, loss = 0.61 (3214.4 images/sec; 0.031 sec/batch)
-----------what is loss : 0.7320712804794312
2019-11-12 15:07:13.207982: step 820, loss = 0.73 (3171.6 images/sec; 0.032 sec/batch)
-----------what is loss : 0.6383873820304871
2019-11-12 15:07:13.326868: step 830, loss = 0.64 (3225.4 images/sec; 0.031 sec/batch)
-----------what is loss : 0.7153497338294983
2019-11-12 15:07:13.445960: step 840, loss = 0.72 (3236.4 images/sec; 0.031 sec/batch)
-----------what is loss : 0.5355468988418579
2019-11-12 15:07:13.565185: step 850, loss = 0.54 (3241.7 images/sec; 0.031 sec/batch)
-----------what is loss : 0.7321954965591431
2019-11-12 15:07:13.685031: step 860, loss = 0.73 (3231.7 images/sec; 0.031 sec/batch)
-----------what is loss : 0.638322651386261
2019-11-12 15:07:13.803861: step 870, loss = 0.64 (3241.7 images/sec; 0.031 sec/batch)
-----------what is loss : 0.6015174388885498
2019-11-12 15:07:13.923730: step 880, loss = 0.60 (3180.5 images/sec; 0.031 sec/batch)
-----------what is loss : 0.7268715500831604
2019-11-12 15:07:14.045314: step 890, loss = 0.73 (3228.6 images/sec; 0.031 sec/batch)
-----------what is loss : 0.7940701246261597
2019-11-12 15:07:14.165552: step 900, loss = 0.79 (3209.2 images/sec; 0.031 sec/batch)
-----------what is loss : 0.7619321942329407
2019-11-12 15:07:14.286328: step 910, loss = 0.76 (3197.6 images/sec; 0.031 sec/batch)
-----------what is loss : 0.6936227083206177
2019-11-12 15:07:14.405373: step 920, loss = 0.69 (3264.7 images/sec; 0.031 sec/batch)
-----------what is loss : 0.7278125882148743
2019-11-12 15:07:14.531524: step 930, loss = 0.73 (2761.9 images/sec; 0.036 sec/batch)
-----------what is loss : 0.504012942314148
2019-11-12 15:07:14.656511: step 940, loss = 0.50 (2780.8 images/sec; 0.036 sec/batch)
-----------what is loss : 0.7674705982208252
2019-11-12 15:07:14.777756: step 950, loss = 0.77 (3161.9 images/sec; 0.032 sec/batch)
-----------what is loss : 0.7070683836936951
2019-11-12 15:07:14.900421: step 960, loss = 0.71 (3155.6 images/sec; 0.032 sec/batch)
-----------what is loss : 0.7509373426437378
2019-11-12 15:07:15.024482: step 970, loss = 0.75 (3181.8 images/sec; 0.031 sec/batch)
-----------what is loss : 0.6206499338150024
2019-11-12 15:07:15.144682: step 980, loss = 0.62 (3246.9 images/sec; 0.031 sec/batch)
-----------what is loss : 0.888460636138916
2019-11-12 15:07:15.265775: step 990, loss = 0.89 (3236.0 images/sec; 0.031 sec/batch)
optimization Finished
Accuracy :  0.8855999994277954
tuple_tem key is dense2
tuple_tem key is dense1
tuple_tem value is (1000, 10)
tuple_tem value is (10,)
tuple_tem value is (784, 1000)
tuple_tem value is (1000,)
tk is  [[ 1.0655087e-06 -3.0114211e-06  0.0000000e+00 ... -3.0114211e-06
   0.0000000e+00  1.0655087e-06]
 [ 6.7243394e-07 -3.0114211e-06  5.1632992e-07 ...  6.7243394e-07
   0.0000000e+00  0.0000000e+00]
 [-3.0114211e-06  6.7243394e-07  5.1632992e-07 ...  2.8214861e-06
   5.1632992e-07  5.1632992e-07]
 ...
 [-1.0369258e-05  1.0118069e-05  1.0118069e-05 ...  6.0853287e-09
   6.0853287e-09  6.0853287e-09]
 [ 6.0853287e-09  1.0118069e-05  1.0118069e-05 ...  6.0853287e-09
  -0.0000000e+00  6.0853287e-09]
 [ 1.0118069e-05  0.0000000e+00  6.0853287e-09 ...  6.0853287e-09
   0.0000000e+00  1.0118069e-05]]
tk is  [[  1   3   4 ...   3   4   1]
 [  5   3   2 ...   5   4   4]
 [  3   5   2 ...   6   2   2]
 ...
 [484 485 485 ... 486 486 486]
 [486 485 485 ... 486 482 486]
 [485 483 486 ... 486 481 485]]
tk is  [[-0.05348052 -0.05348052  0.16462442 ... -0.05348052 -0.05348052
   0.1000633 ]
 [ 0.16462442 -0.          0.1000633  ... -0.         -0.05348052
  -0.05348052]
 [ 0.1000633  -0.05348052 -0.05348052 ... -0.05348052 -0.05348052
  -0.05348052]
 ...
 [-0.         -0.         -0.         ... -0.         -0.
   0.13396336]
 [-0.01254981 -0.01254981 -0.         ... -0.01254981 -0.01254981
  -0.01254981]
 [-0.01254981 -0.01254981 -0.         ... -0.01254981 -0.01254981
  -0.        ]]
tk is  [[ 4  4  5 ...  4  4  1]
 [ 5  3  1 ...  3  4  4]
 [ 1  4  4 ...  4  4  4]
 ...
 [34 34 34 ... 34 34 37]
 [38 38 34 ... 38 38 38]
 [38 38 34 ... 38 38 34]]
tk2 is  True
tk2 is  5
=============================================0.75===========================================
mc.LOAD_PRETRAINED_MODEL is True
OK to save ./MNIST/train/MNIST_PRUNING_INFO.pkl
ncluster/block is 8
nhWeight/block is 13
!!!!!!!!!!!nBlock!!!!! see this !!!61
!!!!!!!!sparse ratio!!!!!!!! see this !!!6.0
blocked!!!
!!!!!!!wantto!!!!!!!!!! see this !!!0.25
!!!!!!!!!but!!!!!! see this !!!0.21620663265306123
1.TKTKTKTK::::::: [[ 1.0655087e-06 -3.0114211e-06  0.0000000e+00 ... -3.0114211e-06
   0.0000000e+00  1.0655087e-06]
 [ 6.7243394e-07 -3.0114211e-06  5.1632992e-07 ...  6.7243394e-07
   0.0000000e+00  0.0000000e+00]
 [-3.0114211e-06  6.7243394e-07  5.1632992e-07 ...  2.8214861e-06
   5.1632992e-07  5.1632992e-07]
 ...
 [-1.0369258e-05  1.0118069e-05  1.0118069e-05 ...  6.0853287e-09
   6.0853287e-09  6.0853287e-09]
 [ 6.0853287e-09  1.0118069e-05  1.0118069e-05 ...  6.0853287e-09
  -0.0000000e+00  6.0853287e-09]
 [ 1.0118069e-05  0.0000000e+00  6.0853287e-09 ...  6.0853287e-09
   0.0000000e+00  1.0118069e-05]]
2.TKTKTKTK::::::: [[ 1.06550874e-06 -0.00000000e+00  0.00000000e+00 ... -0.00000000e+00
   0.00000000e+00  1.06550874e-06]
 [ 6.72433941e-07 -0.00000000e+00  5.16329919e-07 ...  6.72433941e-07
   0.00000000e+00  0.00000000e+00]
 [-0.00000000e+00  6.72433941e-07  5.16329919e-07 ...  2.82148608e-06
   5.16329919e-07  5.16329919e-07]
 ...
 [-0.00000000e+00  1.01180694e-05  1.01180694e-05 ...  6.08532869e-09
   6.08532869e-09  6.08532869e-09]
 [ 6.08532869e-09  1.01180694e-05  1.01180694e-05 ...  6.08532869e-09
  -0.00000000e+00  6.08532869e-09]
 [ 1.01180694e-05  0.00000000e+00  6.08532869e-09 ...  6.08532869e-09
   0.00000000e+00  1.01180694e-05]]
3.TKTKTKTK::::::: [[  1   3   4 ...   3   4   1]
 [  5   3   2 ...   5   4   4]
 [  3   5   2 ...   6   2   2]
 ...
 [484 485 485 ... 486 486 486]
 [486 485 485 ... 486 482 486]
 [485 483 486 ... 486 481 485]]
ncluster/block is 8
nhWeight/block is 200
!!!!!!!!!!!nBlock!!!!! see this !!!5
!!!!!!!!sparse ratio!!!!!!!! see this !!!6.0
blocked!!!
!!!!!!!wantto!!!!!!!!!! see this !!!0.25
!!!!!!!!!but!!!!!! see this !!!0.1851
1.TKTKTKTK::::::: [[-0.05348052 -0.05348052  0.16462442 ... -0.05348052 -0.05348052
   0.1000633 ]
 [ 0.16462442 -0.          0.1000633  ... -0.         -0.05348052
  -0.05348052]
 [ 0.1000633  -0.05348052 -0.05348052 ... -0.05348052 -0.05348052
  -0.05348052]
 ...
 [-0.         -0.         -0.         ... -0.         -0.
   0.13396336]
 [-0.01254981 -0.01254981 -0.         ... -0.01254981 -0.01254981
  -0.01254981]
 [-0.01254981 -0.01254981 -0.         ... -0.01254981 -0.01254981
  -0.        ]]
2.TKTKTKTK::::::: [[-0.05348052 -0.05348052  0.16462442 ... -0.05348052 -0.05348052
   0.1000633 ]
 [ 0.16462442 -0.          0.1000633  ... -0.         -0.05348052
  -0.05348052]
 [ 0.1000633  -0.05348052 -0.05348052 ... -0.05348052 -0.05348052
  -0.05348052]
 ...
 [-0.         -0.         -0.         ... -0.         -0.
   0.13396336]
 [-0.01254981 -0.01254981 -0.         ... -0.01254981 -0.01254981
  -0.01254981]
 [-0.01254981 -0.01254981 -0.         ... -0.01254981 -0.01254981
  -0.        ]]
3.TKTKTKTK::::::: [[ 4  4  5 ...  4  4  1]
 [ 5  3  1 ...  3  4  4]
 [ 1  4  4 ...  4  4  4]
 ...
 [34 34 34 ... 34 34 37]
 [38 38 34 ... 38 38 38]
 [38 38 34 ... 38 38 34]]
debug1.................................................add_forward__graph : end
debug2.................................................add_loss_graph : end
tk:grads_vars is  [(<tf.Tensor 'gradients/AddN_1:0' shape=(784, 1000) dtype=float32>, <tf.Variable 'dense1/weights:0' shape=(784, 1000) dtype=float32_ref>), (<tf.Tensor 'gradients/dense1/BiasAdd_grad/tuple/control_dependency_1:0' shape=(1000,) dtype=float32>, <tf.Variable 'dense1/biases:0' shape=(1000,) dtype=float32_ref>), (<tf.Tensor 'gradients/AddN:0' shape=(1000, 10) dtype=float32>, <tf.Variable 'dense2/weights:0' shape=(1000, 10) dtype=float32_ref>), (<tf.Tensor 'gradients/dense2/BiasAdd_grad/tuple/control_dependency_1:0' shape=(10,) dtype=float32>, <tf.Variable 'dense2/biases:0' shape=(10,) dtype=float32_ref>)]
0 trainable variable is !!!grad: Tensor("gradients/AddN_1:0", shape=(784, 1000), dtype=float32, device=/device:GPU:0)
0 trainable variable is !!!var: <tf.Variable 'dense1/weights:0' shape=(784, 1000) dtype=float32_ref>
1 trainable variable is !!!grad: Tensor("gradients/dense1/BiasAdd_grad/tuple/control_dependency_1:0", shape=(1000,), dtype=float32, device=/device:GPU:0)
1 trainable variable is !!!var: <tf.Variable 'dense1/biases:0' shape=(1000,) dtype=float32_ref>
2 trainable variable is !!!grad: Tensor("gradients/AddN:0", shape=(1000, 10), dtype=float32, device=/device:GPU:0)
2 trainable variable is !!!var: <tf.Variable 'dense2/weights:0' shape=(1000, 10) dtype=float32_ref>
3 trainable variable is !!!grad: Tensor("gradients/dense2/BiasAdd_grad/tuple/control_dependency_1:0", shape=(10,), dtype=float32, device=/device:GPU:0)
3 trainable variable is !!!var: <tf.Variable 'dense2/biases:0' shape=(10,) dtype=float32_ref>
debug3...............................................add_train_graph : end
Successfully Imported ../data/MNIST/train-images.gz
Extracting ../data/MNIST/train-images.gz
Successfully Imported ../data/MNIST/train-labels.gz
Extracting ../data/MNIST/train-labels.gz
Successfully Imported ../data/MNIST/test-images.gz
Extracting ../data/MNIST/test-images.gz
Successfully Imported ../data/MNIST/test-labels.gz
Extracting ../data/MNIST/test-labels.gz
Accuracy :  0.4068999992311001
-----------what is loss : 2.162822723388672
2019-11-12 15:07:19.889653: step 0, loss = 2.16 (1337.7 images/sec; 0.075 sec/batch)
-----------what is loss : 1.5830422639846802
2019-11-12 15:07:20.037757: step 10, loss = 1.58 (3212.3 images/sec; 0.031 sec/batch)
-----------what is loss : 1.4219175577163696
2019-11-12 15:07:20.160181: step 20, loss = 1.42 (3203.4 images/sec; 0.031 sec/batch)
-----------what is loss : 1.275252342224121
2019-11-12 15:07:20.282582: step 30, loss = 1.28 (3260.5 images/sec; 0.031 sec/batch)
-----------what is loss : 1.2345945835113525
2019-11-12 15:07:20.404807: step 40, loss = 1.23 (3192.9 images/sec; 0.031 sec/batch)
-----------what is loss : 1.2299672365188599
2019-11-12 15:07:20.528538: step 50, loss = 1.23 (3157.3 images/sec; 0.032 sec/batch)
-----------what is loss : 1.3272188901901245
2019-11-12 15:07:20.653073: step 60, loss = 1.33 (3090.8 images/sec; 0.032 sec/batch)
-----------what is loss : 1.000733733177185
2019-11-12 15:07:20.778399: step 70, loss = 1.00 (3067.9 images/sec; 0.033 sec/batch)
-----------what is loss : 1.0437843799591064
2019-11-12 15:07:20.901369: step 80, loss = 1.04 (3265.6 images/sec; 0.031 sec/batch)
-----------what is loss : 1.1147122383117676
2019-11-12 15:07:21.022555: step 90, loss = 1.11 (3273.6 images/sec; 0.031 sec/batch)
-----------what is loss : 1.0169130563735962
2019-11-12 15:07:21.145454: step 100, loss = 1.02 (3150.5 images/sec; 0.032 sec/batch)
-----------what is loss : 1.0622140169143677
2019-11-12 15:07:21.266933: step 110, loss = 1.06 (3193.4 images/sec; 0.031 sec/batch)
-----------what is loss : 1.0795834064483643
2019-11-12 15:07:21.388542: step 120, loss = 1.08 (3195.5 images/sec; 0.031 sec/batch)
-----------what is loss : 1.1618539094924927
2019-11-12 15:07:21.511502: step 130, loss = 1.16 (3146.4 images/sec; 0.032 sec/batch)
-----------what is loss : 1.0223758220672607
2019-11-12 15:07:21.635221: step 140, loss = 1.02 (3152.0 images/sec; 0.032 sec/batch)
-----------what is loss : 0.9037743806838989
2019-11-12 15:07:21.758761: step 150, loss = 0.90 (3220.5 images/sec; 0.031 sec/batch)
-----------what is loss : 0.9928148984909058
2019-11-12 15:07:21.879890: step 160, loss = 0.99 (3174.8 images/sec; 0.031 sec/batch)
-----------what is loss : 0.96092689037323
2019-11-12 15:07:22.001490: step 170, loss = 0.96 (3137.8 images/sec; 0.032 sec/batch)
-----------what is loss : 0.9840463399887085
2019-11-12 15:07:22.123337: step 180, loss = 0.98 (3195.1 images/sec; 0.031 sec/batch)
-----------what is loss : 0.9455974102020264
2019-11-12 15:07:22.246910: step 190, loss = 0.95 (3139.6 images/sec; 0.032 sec/batch)
-----------what is loss : 0.9057339429855347
2019-11-12 15:07:22.369157: step 200, loss = 0.91 (3100.6 images/sec; 0.032 sec/batch)
-----------what is loss : 1.0518664121627808
2019-11-12 15:07:22.490950: step 210, loss = 1.05 (3219.2 images/sec; 0.031 sec/batch)
-----------what is loss : 1.1085631847381592
2019-11-12 15:07:22.612912: step 220, loss = 1.11 (3183.6 images/sec; 0.031 sec/batch)
-----------what is loss : 0.8177722096443176
2019-11-12 15:07:22.735131: step 230, loss = 0.82 (3205.3 images/sec; 0.031 sec/batch)
-----------what is loss : 0.9543715715408325
2019-11-12 15:07:22.857491: step 240, loss = 0.95 (3241.0 images/sec; 0.031 sec/batch)
-----------what is loss : 0.9052940607070923
2019-11-12 15:07:22.979112: step 250, loss = 0.91 (3116.8 images/sec; 0.032 sec/batch)
-----------what is loss : 0.7722095847129822
2019-11-12 15:07:23.166867: step 260, loss = 0.77 (1028.9 images/sec; 0.097 sec/batch)
-----------what is loss : 0.9946789145469666
2019-11-12 15:07:23.289886: step 270, loss = 0.99 (3179.6 images/sec; 0.031 sec/batch)
-----------what is loss : 1.0266928672790527
2019-11-12 15:07:23.413332: step 280, loss = 1.03 (3123.1 images/sec; 0.032 sec/batch)
-----------what is loss : 0.8605594635009766
2019-11-12 15:07:23.534231: step 290, loss = 0.86 (3187.0 images/sec; 0.031 sec/batch)
-----------what is loss : 1.0218316316604614
2019-11-12 15:07:23.656580: step 300, loss = 1.02 (3143.9 images/sec; 0.032 sec/batch)
-----------what is loss : 0.9161083102226257
2019-11-12 15:07:23.779307: step 310, loss = 0.92 (3243.0 images/sec; 0.031 sec/batch)
-----------what is loss : 0.8599492311477661
2019-11-12 15:07:23.900173: step 320, loss = 0.86 (3216.4 images/sec; 0.031 sec/batch)
-----------what is loss : 0.8516713976860046
2019-11-12 15:07:24.023381: step 330, loss = 0.85 (3141.6 images/sec; 0.032 sec/batch)
-----------what is loss : 0.9284384250640869
2019-11-12 15:07:24.145582: step 340, loss = 0.93 (3186.6 images/sec; 0.031 sec/batch)
-----------what is loss : 0.8034777641296387
2019-11-12 15:07:24.267473: step 350, loss = 0.80 (3186.4 images/sec; 0.031 sec/batch)
-----------what is loss : 0.8438963890075684
2019-11-12 15:07:24.390292: step 360, loss = 0.84 (3138.6 images/sec; 0.032 sec/batch)
-----------what is loss : 0.8808746337890625
2019-11-12 15:07:24.511849: step 370, loss = 0.88 (3180.8 images/sec; 0.031 sec/batch)
-----------what is loss : 0.8737434148788452
2019-11-12 15:07:24.634747: step 380, loss = 0.87 (3151.1 images/sec; 0.032 sec/batch)
-----------what is loss : 0.7600853443145752
2019-11-12 15:07:24.756142: step 390, loss = 0.76 (3172.0 images/sec; 0.032 sec/batch)
-----------what is loss : 0.9060012102127075
2019-11-12 15:07:24.878070: step 400, loss = 0.91 (3133.5 images/sec; 0.032 sec/batch)
-----------what is loss : 0.825853168964386
2019-11-12 15:07:25.000297: step 410, loss = 0.83 (3145.3 images/sec; 0.032 sec/batch)
-----------what is loss : 0.7862876057624817
2019-11-12 15:07:25.121722: step 420, loss = 0.79 (3215.9 images/sec; 0.031 sec/batch)
-----------what is loss : 0.8483831286430359
2019-11-12 15:07:25.243994: step 430, loss = 0.85 (3117.0 images/sec; 0.032 sec/batch)
-----------what is loss : 1.0189460515975952
2019-11-12 15:07:25.365715: step 440, loss = 1.02 (3207.1 images/sec; 0.031 sec/batch)
-----------what is loss : 0.7166408896446228
2019-11-12 15:07:25.487179: step 450, loss = 0.72 (3188.2 images/sec; 0.031 sec/batch)
-----------what is loss : 0.862517237663269
2019-11-12 15:07:25.608544: step 460, loss = 0.86 (3169.7 images/sec; 0.032 sec/batch)
-----------what is loss : 0.7873806357383728
2019-11-12 15:07:25.730503: step 470, loss = 0.79 (3167.4 images/sec; 0.032 sec/batch)
-----------what is loss : 0.8134669065475464
2019-11-12 15:07:25.853127: step 480, loss = 0.81 (3181.4 images/sec; 0.031 sec/batch)
-----------what is loss : 0.797764778137207
2019-11-12 15:07:25.974936: step 490, loss = 0.80 (3217.2 images/sec; 0.031 sec/batch)
-----------what is loss : 0.8207311630249023
2019-11-12 15:07:26.098134: step 500, loss = 0.82 (3143.7 images/sec; 0.032 sec/batch)
-----------what is loss : 0.9240019917488098
2019-11-12 15:07:26.220833: step 510, loss = 0.92 (3172.0 images/sec; 0.032 sec/batch)
-----------what is loss : 0.7287773489952087
2019-11-12 15:07:26.343476: step 520, loss = 0.73 (3175.0 images/sec; 0.031 sec/batch)
-----------what is loss : 0.764208197593689
2019-11-12 15:07:26.520734: step 530, loss = 0.76 (3181.3 images/sec; 0.031 sec/batch)
-----------what is loss : 0.9034913778305054
2019-11-12 15:07:26.641968: step 540, loss = 0.90 (3214.5 images/sec; 0.031 sec/batch)
-----------what is loss : 0.7718826532363892
2019-11-12 15:07:26.764256: step 550, loss = 0.77 (3181.9 images/sec; 0.031 sec/batch)
-----------what is loss : 0.7515047788619995
2019-11-12 15:07:26.886257: step 560, loss = 0.75 (3222.8 images/sec; 0.031 sec/batch)
-----------what is loss : 0.7867462038993835
2019-11-12 15:07:27.007152: step 570, loss = 0.79 (3234.5 images/sec; 0.031 sec/batch)
-----------what is loss : 0.722820520401001
2019-11-12 15:07:27.129129: step 580, loss = 0.72 (3178.8 images/sec; 0.031 sec/batch)
-----------what is loss : 0.6640065908432007
2019-11-12 15:07:27.250837: step 590, loss = 0.66 (3205.0 images/sec; 0.031 sec/batch)
-----------what is loss : 0.7938245534896851
2019-11-12 15:07:27.372440: step 600, loss = 0.79 (3228.4 images/sec; 0.031 sec/batch)
-----------what is loss : 0.8674097061157227
2019-11-12 15:07:27.493993: step 610, loss = 0.87 (3132.6 images/sec; 0.032 sec/batch)
-----------what is loss : 0.760062038898468
2019-11-12 15:07:27.615209: step 620, loss = 0.76 (3232.8 images/sec; 0.031 sec/batch)
-----------what is loss : 0.8973479270935059
2019-11-12 15:07:27.737024: step 630, loss = 0.90 (3218.1 images/sec; 0.031 sec/batch)
-----------what is loss : 0.9456165432929993
2019-11-12 15:07:27.859211: step 640, loss = 0.95 (3150.4 images/sec; 0.032 sec/batch)
-----------what is loss : 0.9101141095161438
2019-11-12 15:07:27.984206: step 650, loss = 0.91 (3154.2 images/sec; 0.032 sec/batch)
-----------what is loss : 0.6576213836669922
2019-11-12 15:07:28.117004: step 660, loss = 0.66 (3076.4 images/sec; 0.033 sec/batch)
-----------what is loss : 0.6651140451431274
2019-11-12 15:07:28.242474: step 670, loss = 0.67 (3211.6 images/sec; 0.031 sec/batch)
-----------what is loss : 0.7054072618484497
2019-11-12 15:07:28.367917: step 680, loss = 0.71 (3122.7 images/sec; 0.032 sec/batch)
-----------what is loss : 0.8219678997993469
2019-11-12 15:07:28.504296: step 690, loss = 0.82 (2662.2 images/sec; 0.038 sec/batch)
-----------what is loss : 0.9085310101509094
2019-11-12 15:07:28.629872: step 700, loss = 0.91 (3137.5 images/sec; 0.032 sec/batch)
-----------what is loss : 0.7528507709503174
2019-11-12 15:07:28.752580: step 710, loss = 0.75 (3244.5 images/sec; 0.031 sec/batch)
-----------what is loss : 0.7763906717300415
2019-11-12 15:07:28.874020: step 720, loss = 0.78 (3205.6 images/sec; 0.031 sec/batch)
-----------what is loss : 0.7390041351318359
2019-11-12 15:07:28.995830: step 730, loss = 0.74 (3179.3 images/sec; 0.031 sec/batch)
-----------what is loss : 0.8185832500457764
2019-11-12 15:07:29.117650: step 740, loss = 0.82 (3214.6 images/sec; 0.031 sec/batch)
-----------what is loss : 0.934410572052002
2019-11-12 15:07:29.239745: step 750, loss = 0.93 (3199.0 images/sec; 0.031 sec/batch)
-----------what is loss : 0.5424593687057495
2019-11-12 15:07:29.360637: step 760, loss = 0.54 (3230.0 images/sec; 0.031 sec/batch)
-----------what is loss : 0.9085769653320312
2019-11-12 15:07:29.482349: step 770, loss = 0.91 (3202.4 images/sec; 0.031 sec/batch)
-----------what is loss : 0.7015259265899658
2019-11-12 15:07:29.603945: step 780, loss = 0.70 (3202.7 images/sec; 0.031 sec/batch)
-----------what is loss : 0.9557193517684937
2019-11-12 15:07:29.781387: step 790, loss = 0.96 (3169.3 images/sec; 0.032 sec/batch)
-----------what is loss : 0.8882207870483398
2019-11-12 15:07:29.903310: step 800, loss = 0.89 (3172.3 images/sec; 0.032 sec/batch)
-----------what is loss : 0.6479805707931519
2019-11-12 15:07:30.024979: step 810, loss = 0.65 (3218.9 images/sec; 0.031 sec/batch)
-----------what is loss : 0.8146829605102539
2019-11-12 15:07:30.147451: step 820, loss = 0.81 (3227.7 images/sec; 0.031 sec/batch)
-----------what is loss : 1.2311079502105713
2019-11-12 15:07:30.268775: step 830, loss = 1.23 (3200.7 images/sec; 0.031 sec/batch)
-----------what is loss : 0.6625518798828125
2019-11-12 15:07:30.390605: step 840, loss = 0.66 (3177.8 images/sec; 0.031 sec/batch)
-----------what is loss : 0.8786747455596924
2019-11-12 15:07:30.512618: step 850, loss = 0.88 (3198.8 images/sec; 0.031 sec/batch)
-----------what is loss : 0.6986441612243652
2019-11-12 15:07:30.634557: step 860, loss = 0.70 (3219.9 images/sec; 0.031 sec/batch)
-----------what is loss : 0.7205777168273926
2019-11-12 15:07:30.756739: step 870, loss = 0.72 (3183.3 images/sec; 0.031 sec/batch)
-----------what is loss : 0.712970495223999
2019-11-12 15:07:30.879927: step 880, loss = 0.71 (3180.4 images/sec; 0.031 sec/batch)
-----------what is loss : 0.8848706483840942
2019-11-12 15:07:31.002665: step 890, loss = 0.88 (3172.2 images/sec; 0.032 sec/batch)
-----------what is loss : 0.6848678588867188
2019-11-12 15:07:31.124014: step 900, loss = 0.68 (3210.2 images/sec; 0.031 sec/batch)
-----------what is loss : 0.6507267355918884
2019-11-12 15:07:31.246765: step 910, loss = 0.65 (3240.2 images/sec; 0.031 sec/batch)
-----------what is loss : 0.8364525437355042
2019-11-12 15:07:31.367298: step 920, loss = 0.84 (3197.0 images/sec; 0.031 sec/batch)
-----------what is loss : 0.7731979489326477
2019-11-12 15:07:31.489827: step 930, loss = 0.77 (3213.5 images/sec; 0.031 sec/batch)
-----------what is loss : 0.7648431062698364
2019-11-12 15:07:31.612219: step 940, loss = 0.76 (3191.8 images/sec; 0.031 sec/batch)
-----------what is loss : 0.8169577121734619
2019-11-12 15:07:31.733658: step 950, loss = 0.82 (3207.6 images/sec; 0.031 sec/batch)
-----------what is loss : 0.7300616502761841
2019-11-12 15:07:31.855760: step 960, loss = 0.73 (3188.3 images/sec; 0.031 sec/batch)
-----------what is loss : 0.6729391813278198
2019-11-12 15:07:31.977289: step 970, loss = 0.67 (3201.6 images/sec; 0.031 sec/batch)
-----------what is loss : 0.6487536430358887
2019-11-12 15:07:32.101126: step 980, loss = 0.65 (3210.5 images/sec; 0.031 sec/batch)
-----------what is loss : 0.8459650278091431
2019-11-12 15:07:32.224010: step 990, loss = 0.85 (3218.6 images/sec; 0.031 sec/batch)
optimization Finished
Accuracy :  0.8684000003337861
tuple_tem key is dense2
tuple_tem key is dense1
tuple_tem value is (1000, 10)
tuple_tem value is (10,)
tuple_tem value is (784, 1000)
tuple_tem value is (1000,)
tk is  [[ 2.2389302e-06 -0.0000000e+00  3.1069956e-06 ... -0.0000000e+00
   3.1069956e-06  2.2389302e-06]
 [ 3.6003946e-06 -0.0000000e+00 -1.7956087e-07 ...  3.6003946e-06
   3.1069956e-06  3.1069956e-06]
 [-0.0000000e+00  3.6003946e-06 -1.7956087e-07 ...  6.8843328e-06
  -1.7956087e-07 -1.7956087e-07]
 ...
 [-0.0000000e+00  5.5641563e-06  5.5641563e-06 ...  3.3464547e-09
   3.3464547e-09  3.3464547e-09]
 [ 3.3464547e-09  5.5641563e-06  5.5641563e-06 ...  3.3464547e-09
  -0.0000000e+00  3.3464547e-09]
 [ 5.5641563e-06  0.0000000e+00  3.3464547e-09 ...  3.3464547e-09
   0.0000000e+00  5.5641563e-06]]
tk is  [[  1   3   4 ...   3   4   1]
 [  5   3   2 ...   5   4   4]
 [  3   5   2 ...   6   2   2]
 ...
 [484 485 485 ... 486 486 486]
 [486 485 485 ... 486 482 486]
 [485 483 486 ... 486 481 485]]
tk is  [[-0.091026   -0.091026    0.23054186 ... -0.091026   -0.091026
   0.12809446]
 [ 0.23054186  0.00187525  0.12809446 ...  0.00187525 -0.091026
  -0.091026  ]
 [ 0.12809446 -0.091026   -0.091026   ... -0.091026   -0.091026
  -0.091026  ]
 ...
 [-0.08469766 -0.08469766 -0.08469766 ... -0.08469766 -0.08469766
   0.20923586]
 [-0.04180773 -0.04180773 -0.08469766 ... -0.04180773 -0.04180773
  -0.04180773]
 [-0.04180773 -0.04180773 -0.08469766 ... -0.04180773 -0.04180773
  -0.08469766]]
tk is  [[ 4  4  5 ...  4  4  1]
 [ 5  3  1 ...  3  4  4]
 [ 1  4  4 ...  4  4  4]
 ...
 [34 34 34 ... 34 34 37]
 [38 38 34 ... 38 38 38]
 [38 38 34 ... 38 38 34]]
tk2 is  True
tk2 is  5
