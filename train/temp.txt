mc.LOAD_PRETRAINED_MODEL is True
------------------------------------------------
cCentro is  [-0.55642265 -0.55736749 -0.60833878 ... -4.34839149 -2.43918165
 -0.95994303]
cCentro_sum is  [72 80 96 ... 93 80 75]
cCentro is  [-0.00772809 -0.00696709 -0.00633686 ... -0.0467569  -0.03048977
 -0.01279924]
------------------------------------------------
!!!!!!!!!!!!!!!!!! dense1 !!!!!!!!!!!!!!!!!
!!!!!!!!!!!!!!!!!! dense1/weights:0 !!!!!!!!!!!!!!!!!
1.TKTKTKTK::::::: [[-0.02308016 -0.01103532  0.00298919 ...  0.00868712 -0.01027705
  -0.00170626]
 [ 0.01883339 -0.01739853 -0.02318266 ...  0.00074509 -0.00122126
   0.00245491]
 [ 0.00540502 -0.02101506  0.00714282 ...  0.00300029  0.0206655
  -0.01481763]
 ...
 [-0.01648809  0.00266103  0.0052646  ... -0.00347536  0.01533468
   0.00035209]
 [ 0.00434569 -0.01485175 -0.00249617 ... -0.00705109  0.00625652
  -0.00441049]
 [ 0.00177235  0.01875028 -0.00781551 ... -0.0025571   0.01265223
   0.01434736]]
2.TKTKTKTK::::::: [[-0.02072537 -0.03117623 -0.01907929 ... -0.03038453 -0.00236347
   0.00719344]
 [-0.01161827 -0.01773073 -0.040858   ...  0.00430964  0.00309352
  -0.02686546]
 [-0.02346049 -0.00271725  0.00721477 ... -0.02766103 -0.01728147
  -0.02533877]
 ...
 [-0.02540117 -0.00538591 -0.00441023 ... -0.03431847 -0.00910415
  -0.0224612 ]
 [-0.00753082 -0.02016905  0.01248407 ... -0.02581425 -0.00559374
   0.00540713]
 [-0.02772832  0.01397099  0.01148443 ... -0.0018768  -0.00426219
  -0.00870725]]
------------------------------------------------
cCentro is  [ -4.05192098  -3.1546291   -6.2114285   -1.45992109  -6.79867314
  -4.53907629  -6.58707456  -4.61610768  -4.89565184  -6.79612813
  -5.09413296  -4.48171776  -4.96494752  -3.42199701  -4.90650227
  -8.81893321  -5.99189285 -10.60918628  -7.97633505  -3.33586085
  -6.14110634  -1.56776253  -5.15231556  -9.91908517  -5.20553563
  -4.47242354  -8.00322279  -3.16694513  -3.23220589  -2.89777117
  -5.54097457  -5.95806554  -4.41923774  -7.1283798   -5.15751254
  -4.66005938  -4.34578952  -3.18785855  -3.40066881  -4.47923834
  -4.59878545  -0.20994286  -7.951994    -5.01688551  -9.35988873
  -6.29559703  -6.43211497  -5.62729051  -2.4955473   -0.24786017
  -4.40703304  -2.8178386   -5.24140757  -0.737082    -6.77991086
  -9.498495    -0.89685185  -5.060637    -3.39904333  -1.95780598
  -5.02566044  -1.25183811  -3.29268384  -6.65790301  -3.73962575
  -6.32389233  -3.78734841  -4.17572715  -5.82783531  -2.28196323
  -4.65811924  -5.81566809  -3.85156655  -6.52758743  -8.57268595
  -4.01230906  -6.28928289  -7.3564966   -4.03894686  -2.44656424
  -6.01119206  -5.27164839  -6.98833487  -5.05451259  -4.92257832
  -4.72583427  -3.66814074  -7.04976073  -3.73784423  -1.81891714
  -6.13338843  -5.02623684  -1.88913517  -5.52405296  -5.93528665
  -6.00120153  -6.71090278  -3.68162394  -5.53243877  -1.56235759
  -5.18421808  -6.5783215   -3.78216333  -4.90642036  -6.64736173
  -7.79015285  -5.17754192  -6.7025293   -3.87296577  -5.31410091
  -1.99967182  -4.56298419  -5.39379497  -3.67699657  -1.58747903
  -4.6691461   -2.93556765  -2.94860933  -3.62019627  -6.14965967
  -5.91758168  -4.3505451    1.82795838  -2.27167851  -6.26203031]
cCentro_sum is  [ 89  87 102  73  83  89  79  75  90  93  70  79  92  76  71  91  78  85
  79  69  67  85  86  86  85  64  72  84  74  82  84  79  87  70  92  89
  90  91  72  69  77  70  77  83 100  84  79  66  86  67  77  91  61  60
  70 105  80  82  82  74  86  69  67  78  88  91  69  76  95  92  90  92
  88  83  83  64  79  82  80  82  95  79  78  88  79  66  78  80  86  77
  79  65  70  73  89  85  74  91  85  68  81  73  85  84  88  88  64  82
  77  76  86  65  75  80  75  78  98  64  74  74  77  70  64  77  97]
cCentro is  [-0.0455272  -0.0362601  -0.06089636 -0.01999892 -0.08191172 -0.05100086
 -0.08338069 -0.0615481  -0.05439613 -0.07307665 -0.07277333 -0.0567306
 -0.05396682 -0.04502628 -0.06910567 -0.09691135 -0.07681914 -0.12481396
 -0.10096627 -0.04834581 -0.0916583  -0.01844427 -0.05991065 -0.1153382
 -0.0612416  -0.06988162 -0.11115587 -0.03770173 -0.04367846 -0.03533867
 -0.06596398 -0.07541855 -0.05079584 -0.101834   -0.05605992 -0.05236022
 -0.04828655 -0.03503141 -0.04723151 -0.0649165  -0.05972449 -0.00299918
 -0.10327265 -0.0604444  -0.09359889 -0.07494758 -0.08141918 -0.08526198
 -0.02901799 -0.00369941 -0.0572342  -0.03096526 -0.08592471 -0.0122847
 -0.09685587 -0.09046186 -0.01121065 -0.06171509 -0.04145175 -0.02645684
 -0.05843791 -0.01814258 -0.04914453 -0.08535773 -0.04249575 -0.06949332
 -0.05488911 -0.05494378 -0.06134563 -0.02480395 -0.05175688 -0.06321378
 -0.0437678  -0.07864563 -0.10328537 -0.06269233 -0.07961118 -0.08971337
 -0.05048684 -0.02983615 -0.06327571 -0.06672973 -0.08959404 -0.05743764
 -0.06231112 -0.07160355 -0.04702745 -0.08812201 -0.0434633  -0.0236223
 -0.07763783 -0.07732672 -0.02698765 -0.07567196 -0.06668861 -0.07060237
 -0.09068788 -0.04045741 -0.06508751 -0.02297585 -0.06400269 -0.09011399
 -0.04449604 -0.05840977 -0.0755382  -0.08852446 -0.08089909 -0.08173816
 -0.05029826 -0.06992238 -0.023252   -0.07019976 -0.07191727 -0.04596246
 -0.02116639 -0.05986085 -0.02995477 -0.04607202 -0.04892157 -0.08310351
 -0.07685171 -0.06215064  0.02856185 -0.02950232 -0.06455701]
------------------------------------------------
!!!!!!!!!!!!!!!!!! dense2 !!!!!!!!!!!!!!!!!
!!!!!!!!!!!!!!!!!! dense2/weights:0 !!!!!!!!!!!!!!!!!
1.TKTKTKTK::::::: [[-0.24042106 -0.5701208  -0.0037618  ...  0.01765055 -0.13797583
  -0.00983254]
 [-0.00201695  0.06826881 -0.14137879 ... -0.09501888  0.04595011
   0.03995121]
 [-0.41811356 -0.10202878  0.08609141 ... -0.131947    0.16470079
  -0.14783561]
 ...
 [-0.08982211 -0.04945226 -0.17502902 ... -0.02843706  0.18559435
  -0.16309196]
 [ 0.01620742 -0.20754464 -0.33376366 ... -0.5542268  -0.1840653
   0.19819504]
 [ 0.00222358 -0.01743141  0.02367116 ...  0.01192287 -0.00959179
  -0.01438715]]
2.TKTKTKTK::::::: [[-0.06231112 -0.09046186 -0.05972449 ... -0.04828655 -0.04834581
  -0.0122847 ]
 [-0.07307665 -0.05079584 -0.12481396 ... -0.0916583  -0.06400269
  -0.09359889]
 [-0.05029826 -0.0604444  -0.03770173 ... -0.01844427 -0.08191172
  -0.06672973]
 ...
 [-0.05029826 -0.07307665 -0.023252   ... -0.06508751 -0.06949332
  -0.05972449]
 [-0.05029826 -0.0604444  -0.0649165  ... -0.04723151 -0.06400269
  -0.02983615]
 [-0.05840977 -0.02480395 -0.09685587 ... -0.06508751 -0.06321378
  -0.01999892]]
debug1.................................................add_forward__graph : end
debug2.................................................add_loas_graph : end
tk:grads_vars is  [(<tf.Tensor 'gradients/AddN_1:0' shape=(784, 1000) dtype=float32>, <tf.Variable 'dense1/weights:0' shape=(784, 1000) dtype=float32_ref>), (<tf.Tensor 'gradients/dense1/BiasAdd_grad/tuple/control_dependency_1:0' shape=(1000,) dtype=float32>, <tf.Variable 'dense1/biases:0' shape=(1000,) dtype=float32_ref>), (<tf.Tensor 'gradients/AddN:0' shape=(1000, 10) dtype=float32>, <tf.Variable 'dense2/weights:0' shape=(1000, 10) dtype=float32_ref>), (<tf.Tensor 'gradients/dense2/BiasAdd_grad/tuple/control_dependency_1:0' shape=(10,) dtype=float32>, <tf.Variable 'dense2/biases:0' shape=(10,) dtype=float32_ref>)]
0 trainable variable is !!!grad: Tensor("gradients/AddN_1:0", shape=(784, 1000), dtype=float32, device=/device:GPU:0)
0 trainable variable is !!!var: <tf.Variable 'dense1/weights:0' shape=(784, 1000) dtype=float32_ref>
1 trainable variable is !!!grad: Tensor("gradients/dense1/BiasAdd_grad/tuple/control_dependency_1:0", shape=(1000,), dtype=float32, device=/device:GPU:0)
1 trainable variable is !!!var: <tf.Variable 'dense1/biases:0' shape=(1000,) dtype=float32_ref>
2 trainable variable is !!!grad: Tensor("gradients/AddN:0", shape=(1000, 10), dtype=float32, device=/device:GPU:0)
2 trainable variable is !!!var: <tf.Variable 'dense2/weights:0' shape=(1000, 10) dtype=float32_ref>
3 trainable variable is !!!grad: Tensor("gradients/dense2/BiasAdd_grad/tuple/control_dependency_1:0", shape=(10,), dtype=float32, device=/device:GPU:0)
3 trainable variable is !!!var: <tf.Variable 'dense2/biases:0' shape=(10,) dtype=float32_ref>
debug3...............................................add_train_graph : end
Successfully Imported ../data/MNIST/train-images.gz
Extracting ../data/MNIST/train-images.gz
Successfully Imported ../data/MNIST/train-labels.gz
Extracting ../data/MNIST/train-labels.gz
Successfully Imported ../data/MNIST/test-images.gz
Extracting ../data/MNIST/test-images.gz
Successfully Imported ../data/MNIST/test-labels.gz
Extracting ../data/MNIST/test-labels.gz
Accuracy :  0.09740000028163194
-----------what is loss : 2.492079973220825
2019-11-04 09:54:02.816377: step 0, loss = 2.49 (50.7 images/sec; 1.974 sec/batch)
-----------what is loss : 2.4856133460998535
2019-11-04 09:54:22.380369: step 10, loss = 2.49 (50.8 images/sec; 1.968 sec/batch)
-----------what is loss : 2.4764082431793213
2019-11-04 09:54:42.142357: step 20, loss = 2.48 (50.3 images/sec; 1.986 sec/batch)
-----------what is loss : 2.46720552444458
2019-11-04 09:55:01.771145: step 30, loss = 2.47 (51.1 images/sec; 1.957 sec/batch)
-----------what is loss : 2.4818620681762695
2019-11-04 09:55:21.533749: step 40, loss = 2.48 (50.5 images/sec; 1.978 sec/batch)
-----------what is loss : 2.465731382369995
2019-11-04 09:55:41.343424: step 50, loss = 2.47 (51.2 images/sec; 1.954 sec/batch)
-----------what is loss : 2.4648985862731934
2019-11-04 09:56:01.072667: step 60, loss = 2.46 (49.6 images/sec; 2.015 sec/batch)
-----------what is loss : 2.461195468902588
2019-11-04 09:56:20.855300: step 70, loss = 2.46 (50.8 images/sec; 1.968 sec/batch)
-----------what is loss : 2.4642481803894043
2019-11-04 09:56:40.717078: step 80, loss = 2.46 (50.3 images/sec; 1.989 sec/batch)
-----------what is loss : 2.464354991912842
2019-11-04 09:57:00.518982: step 90, loss = 2.46 (50.3 images/sec; 1.987 sec/batch)
-----------what is loss : 2.45072340965271
2019-11-04 09:57:20.297848: step 100, loss = 2.45 (51.4 images/sec; 1.947 sec/batch)
-----------what is loss : 2.4487648010253906
2019-11-04 09:57:40.021011: step 110, loss = 2.45 (50.0 images/sec; 1.999 sec/batch)
-----------what is loss : 2.4519145488739014
2019-11-04 09:57:59.751132: step 120, loss = 2.45 (50.7 images/sec; 1.973 sec/batch)
-----------what is loss : 2.4501547813415527
2019-11-04 09:58:19.509780: step 130, loss = 2.45 (50.5 images/sec; 1.980 sec/batch)
-----------what is loss : 2.4525270462036133
2019-11-04 09:58:39.308002: step 140, loss = 2.45 (50.1 images/sec; 1.997 sec/batch)
-----------what is loss : 2.4520344734191895
2019-11-04 09:58:59.265778: step 150, loss = 2.45 (50.1 images/sec; 1.995 sec/batch)
-----------what is loss : 2.4350879192352295
2019-11-04 09:59:19.000210: step 160, loss = 2.44 (50.7 images/sec; 1.972 sec/batch)
-----------what is loss : 2.4247498512268066
2019-11-04 09:59:38.639727: step 170, loss = 2.42 (50.9 images/sec; 1.964 sec/batch)
-----------what is loss : 2.440873622894287
2019-11-04 09:59:58.357663: step 180, loss = 2.44 (51.2 images/sec; 1.953 sec/batch)
-----------what is loss : 2.4326791763305664
2019-11-04 10:00:17.978937: step 190, loss = 2.43 (51.2 images/sec; 1.951 sec/batch)
-----------what is loss : 2.421118974685669
2019-11-04 10:00:37.647678: step 200, loss = 2.42 (50.9 images/sec; 1.963 sec/batch)
-----------what is loss : 2.429917573928833
2019-11-04 10:00:57.414681: step 210, loss = 2.43 (50.4 images/sec; 1.982 sec/batch)
-----------what is loss : 2.4225471019744873
2019-11-04 10:01:17.106116: step 220, loss = 2.42 (50.5 images/sec; 1.979 sec/batch)
-----------what is loss : 2.417041778564453
2019-11-04 10:01:36.992217: step 230, loss = 2.42 (50.0 images/sec; 2.000 sec/batch)
-----------what is loss : 2.417569160461426
2019-11-04 10:01:56.766435: step 240, loss = 2.42 (50.7 images/sec; 1.972 sec/batch)
-----------what is loss : 2.409682512283325
2019-11-04 10:02:16.484509: step 250, loss = 2.41 (50.5 images/sec; 1.980 sec/batch)
-----------what is loss : 2.422342538833618
2019-11-04 10:02:36.209494: step 260, loss = 2.42 (50.3 images/sec; 1.989 sec/batch)
-----------what is loss : 2.408684253692627
2019-11-04 10:02:55.579736: step 270, loss = 2.41 (51.0 images/sec; 1.961 sec/batch)
-----------what is loss : 2.399968385696411
2019-11-04 10:03:15.083981: step 280, loss = 2.40 (51.2 images/sec; 1.952 sec/batch)
-----------what is loss : 2.398742198944092
2019-11-04 10:03:34.733104: step 290, loss = 2.40 (50.3 images/sec; 1.990 sec/batch)
-----------what is loss : 2.40065598487854
2019-11-04 10:03:54.302261: step 300, loss = 2.40 (50.2 images/sec; 1.992 sec/batch)
-----------what is loss : 2.409052848815918
2019-11-04 10:04:13.854762: step 310, loss = 2.41 (50.9 images/sec; 1.963 sec/batch)
-----------what is loss : 2.402653694152832
2019-11-04 10:04:33.367154: step 320, loss = 2.40 (51.0 images/sec; 1.961 sec/batch)
-----------what is loss : 2.3880550861358643
2019-11-04 10:04:52.980153: step 330, loss = 2.39 (50.7 images/sec; 1.971 sec/batch)
-----------what is loss : 2.3860416412353516
2019-11-04 10:05:12.566673: step 340, loss = 2.39 (50.7 images/sec; 1.971 sec/batch)
-----------what is loss : 2.3850769996643066
2019-11-04 10:05:32.196515: step 350, loss = 2.39 (50.4 images/sec; 1.984 sec/batch)
-----------what is loss : 2.382814407348633
2019-11-04 10:05:51.762846: step 360, loss = 2.38 (51.4 images/sec; 1.945 sec/batch)
-----------what is loss : 2.389793634414673
2019-11-04 10:06:11.340447: step 370, loss = 2.39 (51.5 images/sec; 1.943 sec/batch)
-----------what is loss : 2.3826136589050293
2019-11-04 10:06:30.903320: step 380, loss = 2.38 (50.4 images/sec; 1.984 sec/batch)
-----------what is loss : 2.3976030349731445
2019-11-04 10:06:50.484374: step 390, loss = 2.40 (51.0 images/sec; 1.963 sec/batch)
-----------what is loss : 2.3991334438323975
2019-11-04 10:07:09.986286: step 400, loss = 2.40 (52.3 images/sec; 1.912 sec/batch)
-----------what is loss : 2.3862972259521484
2019-11-04 10:07:29.600961: step 410, loss = 2.39 (51.6 images/sec; 1.938 sec/batch)
-----------what is loss : 2.3893678188323975
2019-11-04 10:07:49.193417: step 420, loss = 2.39 (50.4 images/sec; 1.986 sec/batch)
-----------what is loss : 2.376931667327881
2019-11-04 10:08:08.816556: step 430, loss = 2.38 (51.0 images/sec; 1.962 sec/batch)
-----------what is loss : 2.3840208053588867
2019-11-04 10:08:28.352974: step 440, loss = 2.38 (50.8 images/sec; 1.968 sec/batch)
-----------what is loss : 2.3832788467407227
2019-11-04 10:08:47.977926: step 450, loss = 2.38 (50.5 images/sec; 1.982 sec/batch)
-----------what is loss : 2.3648548126220703
2019-11-04 10:09:07.543324: step 460, loss = 2.36 (50.8 images/sec; 1.967 sec/batch)
-----------what is loss : 2.3664042949676514
2019-11-04 10:09:27.169949: step 470, loss = 2.37 (50.5 images/sec; 1.979 sec/batch)
-----------what is loss : 2.3757214546203613
2019-11-04 10:09:46.780208: step 480, loss = 2.38 (50.7 images/sec; 1.972 sec/batch)
-----------what is loss : 2.3616342544555664
2019-11-04 10:10:06.399734: step 490, loss = 2.36 (50.2 images/sec; 1.992 sec/batch)
-----------what is loss : 2.361504554748535
2019-11-04 10:10:25.904231: step 500, loss = 2.36 (50.2 images/sec; 1.994 sec/batch)
-----------what is loss : 2.3807730674743652
2019-11-04 10:10:45.523872: step 510, loss = 2.38 (50.6 images/sec; 1.978 sec/batch)
-----------what is loss : 2.3349809646606445
2019-11-04 10:11:05.006330: step 520, loss = 2.33 (50.7 images/sec; 1.972 sec/batch)
-----------what is loss : 2.369260549545288
2019-11-04 10:11:24.510281: step 530, loss = 2.37 (52.4 images/sec; 1.910 sec/batch)
-----------what is loss : 2.367544651031494
2019-11-04 10:11:44.078230: step 540, loss = 2.37 (50.8 images/sec; 1.968 sec/batch)
-----------what is loss : 2.353766441345215
2019-11-04 10:12:03.605325: step 550, loss = 2.35 (51.3 images/sec; 1.949 sec/batch)
-----------what is loss : 2.3736846446990967
2019-11-04 10:12:23.044194: step 560, loss = 2.37 (51.8 images/sec; 1.929 sec/batch)
-----------what is loss : 2.3425474166870117
2019-11-04 10:12:42.401722: step 570, loss = 2.34 (51.6 images/sec; 1.938 sec/batch)
-----------what is loss : 2.3713533878326416
2019-11-04 10:13:01.791226: step 580, loss = 2.37 (50.5 images/sec; 1.981 sec/batch)
-----------what is loss : 2.3473668098449707
2019-11-04 10:13:21.292787: step 590, loss = 2.35 (51.2 images/sec; 1.954 sec/batch)
-----------what is loss : 2.3525431156158447
2019-11-04 10:13:40.833292: step 600, loss = 2.35 (50.8 images/sec; 1.969 sec/batch)
-----------what is loss : 2.348477363586426
2019-11-04 10:14:00.390153: step 610, loss = 2.35 (49.9 images/sec; 2.003 sec/batch)
-----------what is loss : 2.3598198890686035
2019-11-04 10:14:19.933149: step 620, loss = 2.36 (51.1 images/sec; 1.958 sec/batch)
-----------what is loss : 2.363481283187866
2019-11-04 10:14:39.525311: step 630, loss = 2.36 (51.5 images/sec; 1.942 sec/batch)
-----------what is loss : 2.3449952602386475
2019-11-04 10:14:59.104431: step 640, loss = 2.34 (50.8 images/sec; 1.969 sec/batch)
-----------what is loss : 2.355661153793335
2019-11-04 10:15:18.624135: step 650, loss = 2.36 (49.4 images/sec; 2.026 sec/batch)
-----------what is loss : 2.3276431560516357
2019-11-04 10:15:38.125953: step 660, loss = 2.33 (51.1 images/sec; 1.958 sec/batch)
-----------what is loss : 2.3434791564941406
2019-11-04 10:15:57.587150: step 670, loss = 2.34 (51.2 images/sec; 1.955 sec/batch)
-----------what is loss : 2.3470513820648193
2019-11-04 10:16:17.123577: step 680, loss = 2.35 (51.2 images/sec; 1.954 sec/batch)
-----------what is loss : 2.331038236618042
2019-11-04 10:16:36.667669: step 690, loss = 2.33 (50.4 images/sec; 1.983 sec/batch)
-----------what is loss : 2.34220814704895
2019-11-04 10:16:56.140064: step 700, loss = 2.34 (50.9 images/sec; 1.966 sec/batch)
-----------what is loss : 2.33115291595459
2019-11-04 10:17:15.726915: step 710, loss = 2.33 (51.4 images/sec; 1.947 sec/batch)
-----------what is loss : 2.309448719024658
2019-11-04 10:17:35.287537: step 720, loss = 2.31 (50.3 images/sec; 1.987 sec/batch)
-----------what is loss : 2.3282899856567383
2019-11-04 10:17:54.784417: step 730, loss = 2.33 (50.3 images/sec; 1.987 sec/batch)
-----------what is loss : 2.289889097213745
2019-11-04 10:18:14.302930: step 740, loss = 2.29 (51.1 images/sec; 1.959 sec/batch)
-----------what is loss : 2.297334909439087
2019-11-04 10:18:33.790899: step 750, loss = 2.30 (50.6 images/sec; 1.977 sec/batch)
-----------what is loss : 2.315993547439575
2019-11-04 10:18:53.257801: step 760, loss = 2.32 (50.6 images/sec; 1.976 sec/batch)
-----------what is loss : 2.291598081588745
2019-11-04 10:19:12.739500: step 770, loss = 2.29 (51.3 images/sec; 1.950 sec/batch)
-----------what is loss : 2.2773592472076416
2019-11-04 10:19:32.266376: step 780, loss = 2.28 (50.8 images/sec; 1.968 sec/batch)
-----------what is loss : 2.282581329345703
2019-11-04 10:19:51.748549: step 790, loss = 2.28 (50.2 images/sec; 1.994 sec/batch)
-----------what is loss : 2.3021175861358643
2019-11-04 10:20:11.273245: step 800, loss = 2.30 (51.2 images/sec; 1.953 sec/batch)
-----------what is loss : 2.2769935131073
2019-11-04 10:20:30.758296: step 810, loss = 2.28 (50.8 images/sec; 1.967 sec/batch)
-----------what is loss : 2.258188247680664
2019-11-04 10:20:50.257420: step 820, loss = 2.26 (51.2 images/sec; 1.952 sec/batch)
-----------what is loss : 2.2126569747924805
2019-11-04 10:21:09.767275: step 830, loss = 2.21 (51.7 images/sec; 1.934 sec/batch)
-----------what is loss : 2.2791967391967773
2019-11-04 10:21:29.513291: step 840, loss = 2.28 (51.1 images/sec; 1.956 sec/batch)
-----------what is loss : 2.263568162918091
2019-11-04 10:21:49.100645: step 850, loss = 2.26 (51.2 images/sec; 1.954 sec/batch)
-----------what is loss : 2.1950085163116455
2019-11-04 10:22:08.678754: step 860, loss = 2.20 (50.9 images/sec; 1.963 sec/batch)
-----------what is loss : 2.2450149059295654
2019-11-04 10:22:28.308227: step 870, loss = 2.25 (51.0 images/sec; 1.961 sec/batch)
-----------what is loss : 2.2266550064086914
2019-11-04 10:22:47.856457: step 880, loss = 2.23 (51.0 images/sec; 1.960 sec/batch)
-----------what is loss : 2.215749502182007
2019-11-04 10:23:07.478059: step 890, loss = 2.22 (51.2 images/sec; 1.953 sec/batch)
-----------what is loss : 2.1853132247924805
2019-11-04 10:23:27.191714: step 900, loss = 2.19 (50.5 images/sec; 1.980 sec/batch)
-----------what is loss : 2.2217307090759277
2019-11-04 10:23:46.818115: step 910, loss = 2.22 (51.6 images/sec; 1.938 sec/batch)
-----------what is loss : 2.194032907485962
2019-11-04 10:24:06.434224: step 920, loss = 2.19 (51.2 images/sec; 1.954 sec/batch)
-----------what is loss : 2.229010820388794
2019-11-04 10:24:26.024789: step 930, loss = 2.23 (52.0 images/sec; 1.924 sec/batch)
-----------what is loss : 2.1975653171539307
2019-11-04 10:24:45.665926: step 940, loss = 2.20 (50.9 images/sec; 1.963 sec/batch)
-----------what is loss : 2.2233433723449707
2019-11-04 10:25:05.299975: step 950, loss = 2.22 (51.9 images/sec; 1.929 sec/batch)
-----------what is loss : 2.0676145553588867
2019-11-04 10:25:25.024403: step 960, loss = 2.07 (52.2 images/sec; 1.917 sec/batch)
-----------what is loss : 2.151430130004883
2019-11-04 10:25:44.646072: step 970, loss = 2.15 (50.9 images/sec; 1.963 sec/batch)
-----------what is loss : 2.1520726680755615
2019-11-04 10:26:04.236154: step 980, loss = 2.15 (51.3 images/sec; 1.949 sec/batch)
-----------what is loss : 2.156998872756958
2019-11-04 10:26:23.809746: step 990, loss = 2.16 (51.3 images/sec; 1.951 sec/batch)
-----------what is loss : 2.151606321334839
2019-11-04 10:26:43.455868: step 1000, loss = 2.15 (50.9 images/sec; 1.963 sec/batch)
-----------what is loss : 2.143596649169922
2019-11-04 10:27:03.007418: step 1010, loss = 2.14 (53.2 images/sec; 1.880 sec/batch)
-----------what is loss : 2.2121877670288086
2019-11-04 10:27:22.542558: step 1020, loss = 2.21 (51.7 images/sec; 1.936 sec/batch)
-----------what is loss : 2.168174982070923
2019-11-04 10:27:41.937323: step 1030, loss = 2.17 (52.0 images/sec; 1.923 sec/batch)
-----------what is loss : 2.146178960800171
2019-11-04 10:28:01.486877: step 1040, loss = 2.15 (51.4 images/sec; 1.946 sec/batch)
-----------what is loss : 2.118643045425415
2019-11-04 10:28:20.996217: step 1050, loss = 2.12 (51.4 images/sec; 1.944 sec/batch)
-----------what is loss : 2.10400128364563
2019-11-04 10:28:40.399479: step 1060, loss = 2.10 (51.7 images/sec; 1.935 sec/batch)
-----------what is loss : 2.1111738681793213
2019-11-04 10:28:59.849513: step 1070, loss = 2.11 (52.0 images/sec; 1.924 sec/batch)
-----------what is loss : 2.100820779800415
2019-11-04 10:29:19.469865: step 1080, loss = 2.10 (52.1 images/sec; 1.920 sec/batch)
-----------what is loss : 2.160679340362549
2019-11-04 10:29:39.065158: step 1090, loss = 2.16 (51.7 images/sec; 1.935 sec/batch)
